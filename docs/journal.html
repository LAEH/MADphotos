<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>MADphotos Dashboard</title>
<link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><circle cx='50' cy='50' r='45' fill='%23111'/></svg>">
<style>
  :root {
    --font-sans: -apple-system, BlinkMacSystemFont, "SF Pro Text", "Helvetica Neue", system-ui, sans-serif;
    --font-display: -apple-system, BlinkMacSystemFont, "SF Pro Display", "Helvetica Neue", system-ui, sans-serif;
    --font-mono: "SF Mono", ui-monospace, "Cascadia Code", monospace;
    --text-xs: 11px; --text-sm: 13px; --text-base: 15px; --text-lg: 17px;
    --text-xl: 20px; --text-2xl: 22px; --text-3xl: 28px;
    --space-1: 4px; --space-2: 8px; --space-3: 12px; --space-4: 16px;
    --space-5: 20px; --space-6: 24px; --space-8: 32px; --space-10: 40px;
    --radius-sm: 6px; --radius-md: 10px; --radius-lg: 14px;
    --sidebar-w: 220px;
    --apple-blue: #007AFF; --apple-green: #34C759;
    --leading-normal: 1.47; --leading-relaxed: 1.6;
    --tracking-tight: -0.01em; --tracking-caps: 0.06em;
    --duration-fast: 150ms; --ease-default: cubic-bezier(0.25, 0.1, 0.25, 1);
    --apple-purple: #AF52DE; --apple-pink: #FF2D55; --apple-orange: #FF9500;
  }
  [data-theme="light"] {
    --bg: #F5F5F7; --bg-secondary: #FFFFFF; --fg: #1D1D1F;
    --fg-secondary: #3A3A3C; --muted: #86868B;
    --border: rgba(0,0,0,0.08); --border-strong: rgba(0,0,0,0.14);
    --card-bg: #FFFFFF; --sidebar-bg: #FFFFFF;
    --sidebar-active-bg: rgba(0,0,0,0.04); --hover-overlay: rgba(0,0,0,0.03);
    color-scheme: light;
  }
  [data-theme="dark"] {
    --bg: #1C1C1E; --bg-secondary: #2C2C2E; --fg: #F5F5F7;
    --fg-secondary: #D1D1D6; --muted: #98989D;
    --border: rgba(255,255,255,0.08); --border-strong: rgba(255,255,255,0.14);
    --card-bg: #2C2C2E; --sidebar-bg: #2C2C2E;
    --sidebar-active-bg: rgba(255,255,255,0.06); --hover-overlay: rgba(255,255,255,0.04);
    color-scheme: dark;
  }
  @keyframes ai-gradient {
    0% { background-position: 0% 50%; }
    50% { background-position: 100% 50%; }
    100% { background-position: 0% 50%; }
  }
  @keyframes fade-up {
    from { opacity: 0; transform: translateY(12px); }
    to { opacity: 1; transform: translateY(0); }
  }
  html { scroll-behavior: smooth; }
  *, *::before, *::after { margin: 0; padding: 0; box-sizing: border-box; }
  body {
    font-family: var(--font-sans); font-size: var(--text-base);
    line-height: var(--leading-normal); background: var(--bg); color: var(--fg);
    display: flex; min-height: 100vh;
    -webkit-font-smoothing: antialiased;
  }
  .sidebar {
    width: var(--sidebar-w); min-width: var(--sidebar-w);
    background: var(--sidebar-bg); border-right: 1px solid var(--border);
    padding: var(--space-5) 0; position: sticky; top: 0; height: 100vh;
    overflow-y: auto; display: flex; flex-direction: column;
    transition: width var(--duration-normal) var(--ease-default),
                min-width var(--duration-normal) var(--ease-default);
    position: relative;
  }
  .sidebar::after {
    content: '';
    position: absolute; top: 0; right: 0; width: 2px; height: 100%;
    background: linear-gradient(180deg, var(--apple-blue) 0%, var(--apple-purple) 35%, var(--apple-pink) 65%, var(--apple-orange) 100%);
    opacity: 0.35; background-size: 100% 300%; animation: ai-gradient 8s ease infinite;
  }
  .sidebar .sb-title {
    font-family: var(--font-display); font-size: var(--text-lg); font-weight: 700;
    letter-spacing: var(--tracking-tight); padding: 0 var(--space-5) var(--space-4);
    border-bottom: 1px solid var(--border); margin-bottom: var(--space-2);
  }
  .sidebar a {
    display: flex; align-items: center; gap: var(--space-2);
    padding: var(--space-2) var(--space-5); color: var(--muted);
    text-decoration: none; font-size: var(--text-sm);
    transition: background var(--duration-fast) var(--ease-default),
                color var(--duration-fast) var(--ease-default);
  }
  .sidebar a:hover { background: var(--sidebar-active-bg); color: var(--fg); }
  .sidebar a.active {
    color: var(--fg); font-weight: 600; background: var(--sidebar-active-bg);
  }
  .sidebar a.sb-sub { padding-left: var(--space-8); font-size: var(--text-xs); }
  .sidebar .sb-sep { height: 1px; background: var(--border); margin: var(--space-2) var(--space-5); }
  .sidebar .sb-group {
    font-size: var(--text-xs); font-weight: 600; text-transform: uppercase;
    letter-spacing: var(--tracking-caps); color: var(--muted);
    padding: var(--space-3) var(--space-5) var(--space-1);
  }
  .sidebar .sb-bottom {
    margin-top: auto; padding: var(--space-3) var(--space-5);
    border-top: 1px solid var(--border);
  }
  .theme-toggle {
    display: flex; align-items: center; gap: var(--space-2);
    font-size: var(--text-sm); color: var(--muted); cursor: pointer;
    padding: var(--space-2) 0; background: none; border: none;
    font-family: inherit; width: 100%;
    transition: color var(--duration-fast);
  }
  .theme-toggle:hover { color: var(--fg); }
  .theme-toggle .theme-icon { font-size: 16px; }
  .sb-collapse {
    display: flex; align-items: center; gap: var(--space-2);
    font-size: var(--text-sm); color: var(--muted); cursor: pointer;
    padding: var(--space-2) 0; background: none; border: none;
    font-family: inherit; width: 100%; transition: color var(--duration-fast);
    margin-bottom: var(--space-2); white-space: nowrap; overflow: hidden;
  }
  .sb-collapse:hover { color: var(--fg); }
  .sb-expand {
    display: none; position: fixed; top: var(--space-4); left: var(--space-4);
    z-index: 50; width: 36px; height: 36px; border-radius: var(--radius-sm);
    border: 1px solid var(--border); background: var(--card-bg);
    cursor: pointer; color: var(--muted); box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    align-items: center; justify-content: center; font-size: 18px;
    transition: color var(--duration-fast), box-shadow var(--duration-fast);
  }
  .sb-expand:hover { color: var(--fg); box-shadow: 0 4px 16px rgba(0,0,0,0.15); }
  body.sb-collapsed .sidebar { width: 0; min-width: 0; overflow: hidden; padding: 0; border-right: none; }
  body.sb-collapsed .sb-expand { display: flex; }
  .sb-hamburger {
    display: none; align-items: center; justify-content: center;
    width: 36px; height: 36px; background: none; border: none;
    cursor: pointer; color: var(--fg); font-size: 20px;
    border-radius: var(--radius-sm); flex-shrink: 0;
  }
  .main-content {
    flex: 1; padding: var(--space-10) var(--space-8);
    max-width: 900px; min-width: 0; margin: 0 auto;
    animation: fade-up 0.5s var(--ease-default) both;
  }
  @media (max-width: 900px) {
    body { flex-direction: column; }
    body.sb-collapsed .sidebar {
      width: 100%; min-width: unset; overflow: visible; padding: var(--space-2) var(--space-3);
      border-right: none; border-bottom: 1px solid var(--border);
    }
    body.sb-collapsed .sb-expand { display: none; }
    .sb-collapse { display: none !important; }
    .sidebar { width: 100%; min-width: unset; height: auto; position: sticky; top: 0;
               z-index: 100; border-right: none; border-bottom: 1px solid var(--border);
               flex-direction: row; flex-wrap: wrap; padding: var(--space-2) var(--space-3);
               backdrop-filter: blur(20px); -webkit-backdrop-filter: blur(20px);
               background: color-mix(in srgb, var(--sidebar-bg) 85%, transparent); }
    .sidebar .sb-title { width: auto; border-bottom: none; padding: 0 var(--space-2) 0 0; margin-bottom: 0; font-size: var(--text-base); }
    .sb-hamburger { display: flex; margin-left: auto; }
    .sidebar > a, .sidebar .sb-group, .sidebar .sb-sep, .sidebar .sb-bottom { display: none; }
    .sidebar.open > a { display: flex; width: 100%; }
    .sidebar.open .sb-sep { display: block; width: 100%; }
    .sidebar.open .sb-bottom { display: block; width: 100%; }
    .main-content { padding: var(--space-6); }
  }
  h1 { font-family: var(--font-display); font-size: var(--text-3xl); font-weight: 700;
       letter-spacing: var(--tracking-tight); margin-bottom: var(--space-2); }
  h2 { font-family: var(--font-display); font-size: var(--text-xl); font-weight: 700;
       margin: var(--space-8) 0 var(--space-3); color: var(--fg);
       border-bottom: 1px solid var(--border); padding-bottom: var(--space-2); }
  h3 { font-size: var(--text-lg); font-weight: 600; margin: var(--space-6) 0 var(--space-2); color: var(--fg); }
  h4 { font-size: var(--text-base); font-weight: 600; margin: var(--space-4) 0 var(--space-2); color: var(--muted); }
  p { font-size: var(--text-sm); margin: var(--space-2) 0; line-height: var(--leading-relaxed); }
  ul { font-size: var(--text-sm); margin: var(--space-2) 0 var(--space-2) var(--space-5); }
  li { margin: var(--space-1) 0; }
  table { width: 100%; border-collapse: collapse; font-size: var(--text-sm); margin: var(--space-3) 0 var(--space-4); }
  th { text-align: left; font-size: var(--text-xs); font-weight: 600; text-transform: uppercase;
       letter-spacing: var(--tracking-caps); color: var(--muted);
       border-bottom: 1px solid var(--border-strong); padding: var(--space-2) var(--space-3); }
  td { padding: var(--space-2) var(--space-3); border-bottom: 1px solid var(--border); }
  code { background: var(--hover-overlay); padding: 2px var(--space-2); font-size: var(--text-sm);
         font-family: var(--font-mono); border-radius: var(--radius-sm); }
  strong { font-weight: 700; }
  a { color: var(--fg); }
  blockquote { font-size: var(--text-sm); color: var(--muted); border-left: 3px solid var(--border);
               padding-left: var(--space-4); margin: var(--space-2) 0; font-style: normal; }
  footer { margin-top: var(--space-10); padding-top: var(--space-4);
           border-top: 1px solid var(--border); font-size: var(--text-xs); color: var(--muted); }
  footer a { color: var(--muted); text-decoration: none; }
  
</style>
</head>
<body>
<button class="sb-expand" onclick="toggleSidebar()" title="Show sidebar">&#9776;</button>
<nav class="sidebar" id="sidebar">
  <div class="sb-title">MADphotos</div>
  <button class="sb-hamburger" onclick="document.getElementById('sidebar').classList.toggle('open')" aria-label="Menu">&#9776;</button>
  <a href="state.html">State</a>
  <a href="journal.html" class="active">Journal de Bord</a>
  <a href="instructions.html">System Instructions</a>
  <div class="sb-sep"></div>
  <div class="sb-group">Experiments</div>
  <a href="drift.html">Similarity</a>
  <a href="creative-drift.html">Drift</a>
  <a href="blind-test.html">Blind Test</a>
  <a href="mosaics.html">Mosaics</a>
  <div class="sb-sep"></div>
  <div class="sb-bottom">
    <button class="sb-collapse" onclick="toggleSidebar()">&#x276E; Hide sidebar</button>
    <button class="theme-toggle" onclick="toggleTheme()" id="themeBtn">
      <span class="theme-icon" id="themeIcon">&#9790;</span>
      <span id="themeLabel">Dark Mode</span>
    </button>
  </div>
</nav>
<div class="main-content">
<style>
  .date-header {
    font-size: var(--text-sm); font-weight: 600; margin: var(--space-8) 0 var(--space-3);
    padding: var(--space-2) var(--space-3); color: var(--muted);
    background: var(--hover-overlay); border-radius: var(--radius-sm);
    letter-spacing: var(--tracking-caps); text-transform: uppercase;
  }
  .event {
    background: var(--card-bg);
    border: 1px solid var(--border);
    border-radius: var(--radius-md);
    padding: var(--space-4) var(--space-5);
    margin-bottom: var(--space-3);
    transition: border-color var(--duration-fast) var(--ease-default);
    position: relative;
  }
  .event:hover {
    border-color: var(--border-strong);
  }
  .event-genesis {
    border-color: var(--apple-indigo);
    background: linear-gradient(135deg, var(--card-bg) 0%, rgba(88,86,214,0.06) 100%);
  }
  .event-genesis h3 {
    font-size: var(--text-base) !important; font-weight: 800;
    letter-spacing: -0.01em;
  }
  /* Thread connector line */
  .event + .event::before {
    content: "";
    position: absolute;
    top: calc(-1 * var(--space-3));
    left: var(--space-6);
    width: 2px;
    height: var(--space-3);
    background: var(--border);
  }
  /* Event type labels */
  .ev-labels {
    display: flex; gap: 6px; margin-bottom: 6px; flex-wrap: wrap;
  }
  .ev-label {
    font-size: 10px; font-weight: 600; text-transform: uppercase;
    letter-spacing: 0.05em;
    padding: 2px 8px; border-radius: var(--radius-full);
    color: var(--label-color);
    background: color-mix(in srgb, var(--label-color) 12%, transparent);
    border: 1px solid color-mix(in srgb, var(--label-color) 25%, transparent);
    line-height: 1.4;
  }
  .main-content h3 {
    font-size: var(--text-sm); font-weight: 700; margin: 0;
    color: var(--fg); display: block; line-height: var(--leading-normal);
  }
  .quote {
    font-size: var(--text-xs); color: var(--muted); font-style: italic;
    font-weight: 400; display: block; margin-top: 2px;
  }
  /* Compact/expanded toggle */
  .event { cursor: pointer; }
  .ev-expand-hint {
    font-size: 10px; color: var(--muted); transition: transform 0.2s;
    display: inline-block; margin-left: 4px;
  }
  .ev-collapsed .ev-body { display: none; }
  .ev-collapsed .ev-summary { display: block; }
  .event:not(.ev-collapsed) .ev-body { display: block; }
  .event:not(.ev-collapsed) .ev-summary { display: none; }
  .event:not(.ev-collapsed) .ev-expand-hint { transform: rotate(90deg); }
  .ev-summary {
    font-size: var(--text-sm); color: var(--muted);
    margin-top: var(--space-1); line-height: var(--leading-relaxed);
  }
  .ev-summary p { margin: 0; }
  .event p {
    font-size: var(--text-sm); color: var(--fg-secondary);
    margin: var(--space-1) 0; line-height: var(--leading-relaxed);
  }
  .event ul { list-style: none; margin: var(--space-2) 0; padding: 0; }
  .event li {
    font-size: var(--text-sm); color: var(--fg-secondary);
    padding: var(--space-1) 0 var(--space-1) var(--space-5); position: relative;
    line-height: var(--leading-relaxed);
  }
  .event li::before { content: "—"; position: absolute; left: 0; color: var(--muted); }
  .event pre {
    background: var(--hover-overlay); border-radius: var(--radius-sm);
    padding: var(--space-3); margin: var(--space-2) 0; overflow-x: auto;
    font-size: 11px; line-height: 1.5;
  }
  .event code { font-family: var(--font-mono); font-size: 0.9em; }
  .event .table-wrap { overflow-x: auto; margin: var(--space-2) 0; }
  .event table {
    width: 100%; border-collapse: collapse; font-size: var(--text-xs);
  }
  .event th, .event td {
    padding: var(--space-1) var(--space-2); text-align: left;
    border-bottom: 1px solid var(--border);
  }
  .event th { font-weight: 600; color: var(--fg); }
  .main-content p { font-size: var(--text-sm); color: var(--fg-secondary); margin-bottom: var(--space-2); line-height: var(--leading-relaxed); }
  .main-content ul { list-style: none; margin: var(--space-3) 0; }
  .main-content li {
    font-size: var(--text-sm); color: var(--fg-secondary);
    padding: var(--space-1) 0 var(--space-1) var(--space-5); position: relative;
  }
  .main-content li::before { content: "—"; position: absolute; left: 0; color: var(--muted); }
  hr { border: none; margin: 0; }
</style>
<h2 class="date-header">2026-02-07</h2>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>08:45 — Show: Full Verification Pass <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>All 14 Show experiences verified as implemented and functional. Re-exported gallery data: 9,011 photos, 8,401 Gemini analyses, 1,676 face detections, 200 game rounds, 8,618 stream sequence entries. Every experience module (La Grille, Le Bento, La Similarite, La Derive, Les Couleurs, Le Terrain de Jeu, La Chambre Noire, Le Flot, Les Visages, La Boussole, L'Observatoire, La Carte, La Machine a Ecrire, Le Pendule) has a real implementation with proper CSS design system integration. The 2,077-line style.css covers all experiences with Apple HIG tokens, dark mode, immersive views, and accessibility (reduced motion). Gallery served locally via <code>serve_gallery.py</code> on port 3000, with <code>/rendered/</code> proxy for local tier images.</p></div><div class="ev-body"><p>All 14 Show experiences verified as implemented and functional. Re-exported gallery data: 9,011 photos, 8,401 Gemini analyses, 1,676 face detections, 200 game rounds, 8,618 stream sequence entries. Every experience module (La Grille, Le Bento, La Similarite, La Derive, Les Couleurs, Le Terrain de Jeu, La Chambre Noire, Le Flot, Les Visages, La Boussole, L'Observatoire, La Carte, La Machine a Ecrire, Le Pendule) has a real implementation with proper CSS design system integration. The 2,077-line style.css covers all experiences with Apple HIG tokens, dark mode, immersive views, and accessibility (reduced motion). Gallery served locally via <code>serve_gallery.py</code> on port 3000, with <code>/rendered/</code> proxy for local tier images.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>08:30 — Gemini Re-Analysis: 633 Images <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Discovered 633 images still needed Gemini analysis: 7 entirely missing, 626 with stale "reauthentication needed" errors from an expired OAuth session. Re-authenticated with <code>gcloud auth application-default login</code> and launched <code>photography_engine.py</code>. The engine has built-in retry with exponential backoff, and immediately started processing despite hitting Vertex AI rate limits (429 RESOURCE_EXHAUSTED). Current state: 8,439 good analyses out of 9,011.</p></div><div class="ev-body"><p>Discovered 633 images still needed Gemini analysis: 7 entirely missing, 626 with stale "reauthentication needed" errors from an expired OAuth session. Re-authenticated with <code>gcloud auth application-default login</code> and launched <code>photography_engine.py</code>. The engine has built-in retry with exponential backoff, and immediately started processing despite hitting Vertex AI rate limits (429 RESOURCE_EXHAUSTED). Current state: 8,439 good analyses out of 9,011.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>00:40 — State App: AI-Alive Design Update <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Applied the AI-native design language to the State dashboard (<code>generate_status_page.py</code>). The State app now has the same sense of intelligence as Show.</p></div><div class="ev-body"><p>Applied the AI-native design language to the State dashboard (<code>generate_status_page.py</code>). The State app now has the same sense of intelligence as Show.</p>
<p><strong>Sidebar AI accent</strong>: Added a 2px animated gradient line (blue→purple→pink→orange) on the sidebar's right edge via <code>::after</code>, cycling with <code>ai-gradient</code> at 35% opacity. Both the main status CSS and the <code>page_shell()</code> shared layout get it.</p>
<p><strong>Animations</strong>: Added <code>@keyframes ai-shimmer</code>, <code>fade-up</code>, and <code>ai-gradient</code> to both CSS contexts. State hero gets <code>fade-up 0.6s</code>. Main content areas animate in with <code>fade-up 0.5s</code>. Element grid cards stagger their entrance (4 groups, 60ms increments).</p>
<p><strong>Section title accent</strong>: Every <code>.section-title</code> gets a 60px gradient underline (blue→purple) at 60% opacity via <code>::after</code> — a subtle visual signature.</p>
<p><strong>Token migration</strong>: Fixed 4 hardcoded camera tag icon colors (<code>#86868b</code>, <code>#a1a1a6</code>, <code>#6e6e73</code>, <code>#98989d</code>) → <code>var(--muted)</code> / <code>var(--fg-secondary)</code>. Fixed <code>METHOD_COLORS</code> in blind-test JS from hardcoded hex to runtime <code>getComputedStyle()</code> reads of <code>--muted</code>, <code>--apple-blue</code>, <code>--apple-green</code>. Fixed skipped-row color from <code>#86868B</code> → <code>var(--muted)</code>.</p>
<p>Regenerated all 6 static HTML pages (state, journal, instructions, drift, blind-test, mosaics).</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-orange)">Investigation</span></div><h3>00:30 — Hardcoded Style Purge: 14 JS Files Fixed <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Audited all 15 JS experience modules for styles that bypass the CSS design system. Found 24 violations across 8 files — 14 high-priority, 8 medium, 2 low.</p></div><div class="ev-body"><p>Audited all 15 JS experience modules for styles that bypass the CSS design system. Found 24 violations across 8 files — 14 high-priority, 8 medium, 2 low.</p>
<p><strong>grid.js</strong>: Replaced two inline flex/wrap/gap blocks with <code>.grid-overlay-tags</code> and <code>.filter-active-section</code> CSS classes.</p>
<p><strong>colors.js</strong>: Four fixes — grid inline styles → <code>.colors-grid</code> / <code>.colors-grid-bucket</code> classes, swatch 6-line inline styles → <code>.color-swatch-sm</code> class, empty state inline styles → <code>.empty-state</code> class.</p>
<p><strong>pendulum.js</strong>: Replaced <code>style.fontSize = '28px'</code> with <code>.pendulum-results-title</code> CSS class.</p>
<p><strong>drift.js / similarity.js / observatory.js</strong>: Replaced <code>style.cursor = 'pointer'</code> with shared <code>.clickable-img</code> CSS class.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span><span class="ev-label" style="--label-color:var(--apple-pink)">UI/UX</span></div><h3>00:15 — Show: Light-First Design System Rewrite <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Complete rewrite of <code>web/style.css</code> — flipped from dark-only to a light-first design system with proper <code>@media (prefers-color-scheme: dark)</code>. Photography-immersive experiences (Le Bento, La Chambre Noire, Le Flot, La Carte) keep forced-dark via CSS custom property overrides on their container elements. Everything else gets clean, bright Apple-style surfaces in light mode.</p></div><div class="ev-body"><p>Complete rewrite of <code>web/style.css</code> — flipped from dark-only to a light-first design system with proper <code>@media (prefers-color-scheme: dark)</code>. Photography-immersive experiences (Le Bento, La Chambre Noire, Le Flot, La Carte) keep forced-dark via CSS custom property overrides on their container elements. Everything else gets clean, bright Apple-style surfaces in light mode.</p>
<p><strong>New token layer</strong>: <code>--fill-primary/secondary/tertiary/quaternary</code> (Apple Fill Colors), <code>--shadow-sm/md/lg</code> (light vs dark shadow), <code>--header-bg</code> (frosted header), <code>--separator</code>, <code>--bg-tertiary</code>. Typography switched from monospace to system font as default. Radius bumped to 12px/8px. All surfaces, glass layers, and fills now respect both modes.</p>
<p><strong>AI-alive animations</strong>: Added <code>@keyframes ai-shimmer</code> (traveling gradient), <code>ai-gradient</code> (cycling color gradient), <code>fade-up</code> (entrance reveal), <code>ai-pulse</code> (subtle breathing). Header gets a traveling blue→purple→pink accent line via <code>#header::after</code>. Loading indicator changed from spinner text to a shimmer gradient bar. Launcher cards cascade in with staggered <code>animation-delay</code>. Every view transition gets a <code>fade-up</code> entrance. Lazy images fade in via CSS attribute selector (<code>img[data-src] { opacity: 0 }</code>).</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>03:45 — State Dashboard: Sidebar Fix <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Fixed the broken "Gemini Progress" link in the State dashboard sidebar — the <code>#sec-gemini</code> anchor didn't exist on the page. Added <code>id="sec-gemini"</code> to the Models section. Restructured the sidebar so dashboard sub-items (Models, Signals, Vector Store, Camera Fleet, Render Tiers, Storage, Pipeline Runs, Sample Output) are now nested under a collapsible "State" group instead of being in a separate "Dashboard" section. Added <code>.sb-sub</code> CSS class for indented sub-navigation.</p></div><div class="ev-body"><p>Fixed the broken "Gemini Progress" link in the State dashboard sidebar — the <code>#sec-gemini</code> anchor didn't exist on the page. Added <code>id="sec-gemini"</code> to the Models section. Restructured the sidebar so dashboard sub-items (Models, Signals, Vector Store, Camera Fleet, Render Tiers, Storage, Pipeline Runs, Sample Output) are now nested under a collapsible "State" group instead of being in a separate "Dashboard" section. Added <code>.sb-sub</code> CSS class for indented sub-navigation.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span></div><h3>03:30 — Apple System Colors: Full Design System Migration <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Replaced every custom color in the Show web gallery with Apple's official system color palette. The app now has a single source of truth: 12 vibrant colors + 6 grays from Apple HIG, with automatic dark/light adaptation via <code>prefers-color-scheme</code>.</p></div><div class="ev-body"><p>Replaced every custom color in the Show web gallery with Apple's official system color palette. The app now has a single source of truth: 12 vibrant colors + 6 grays from Apple HIG, with automatic dark/light adaptation via <code>prefers-color-scheme</code>.</p>
<p><strong><code>:root</code></strong>: Replaced all 8 custom category colors (<code>--c-vibe</code>, <code>--c-grading</code>, etc.) with references to system colors (<code>var(--system-orange)</code>, <code>var(--system-blue)</code>, etc.). Same for 8 emotion colors. Added <code>@media (prefers-color-scheme: dark)</code> with Apple's dark-mode hue shifts — the blue in dark is NOT the blue in light. The depth layer colors now use Apple cyan/green/red. Even the <code>--bg-elevated</code> and <code>--text</code> values aligned to Apple's gray scale.</p>
<p><strong>Glass tags</strong>: All 8 category tag styles migrated from hardcoded <code>rgba()</code> to <code>color-mix(in srgb, var(--system-*) X%, transparent)</code>. This means when system colors shift between light/dark mode, every tag automatically adapts. No more maintaining parallel color values.</p>
<p><strong>Badges, game buttons, observatory bars</strong>: All hardcoded hex colors replaced with system color references. Only one hex remains in the entire CSS: <code>--bg: #0a0a0a</code> — the photography app's true black background, intentionally darker than Apple's deepest gray.</p>
<p><strong><code>colors.js</code></strong>: The <code>colorNameToHex()</code> function became <code>colorNameToCSS()</code> — it now reads Apple system colors from CSS variables at runtime. The gray bucket color reads <code>--system-gray</code>.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>03:00 — Design Token Audit: Hardcoded Styles → CSS Variables <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Scanned all 15 JS files for hardcoded styles that bypass the design system. Found 34 style assignments — 9 needed migration.</p></div><div class="ev-body"><p>Scanned all 15 JS files for hardcoded styles that bypass the design system. Found 34 style assignments — 9 needed migration.</p>
<p>Added 13 new CSS tokens: <code>--emo-happy/sad/angry/surprise/fear/disgust/neutral/contempt</code> (emotion colors), <code>--color-error</code> (error red), <code>--depth-near/mid/far</code> (depth layer visualization). Migrated <code>faces.js</code> from hardcoded <code>EMOTION_COLORS</code> map to <code>emoColor()</code> that reads from CSS variables. Replaced inline <code>style="color:#ef5350"</code> in app.js error states with <code>.loading.error</code> CSS class. Migrated darkroom depth bars from hardcoded <code>rgba()</code> to <code>var(--depth-*)</code>. Made map canvas read <code>--bg</code> and <code>--glass-border</code> from CSS tokens. The remaining 25 assignments are legitimate dynamic values (computed widths, animation states, layout calculations) that can't be static tokens.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>02:30 — Expert Team: Experience Module Fixes <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Swept all 14 experience modules for the same issues.</p></div><div class="ev-body"><p>Swept all 14 experience modules for the same issues.</p>
<p><strong>game.js</strong>: Replaced <code>setInterval(50ms)</code> (20fps!) with <code>requestAnimationFrame</code> for the timer bar — now silky smooth at 60fps. Uses <code>performance.now()</code> for precise timing. Added <code>answered</code> guard preventing double-click exploits. Progressive image loading for game photos.</p>
<p><strong>bento.js</strong>: Registered crossfade interval with <code>registerTimer()</code> so it gets cleaned up on view switch. Crossfade now uses <code>transitionend</code> event instead of blind <code>setTimeout(800)</code> — respects actual CSS transition timing and <code>prefers-reduced-motion</code>. Uses <code>loadProgressive()</code> for display-tier images.</p>
<p><strong>grid.js</strong>: Added debounced filter rendering (80ms) so rapid tag clicks don't trigger 5000-photo re-layout per click. Cached <code>gridLastVisible</code> array eliminates double-filtering for count display.</p>
<p><strong>All modules</strong>: Removed stale <code>*Initialized</code> flags from all 11 experience modules. These prevented re-initialization when navigating back to an experience, causing stale state. Now every experience rebuilds fresh on entry, and <code>clearAllTimers()</code> handles cleanup.</p>
<p><strong>compass.js</strong>: Replaced <code>shuffleArray([...all]).slice(0, 500)</code> (copies entire 9k array to sample 500) with stride-based sampling — zero allocations.</p>
<p><strong>map.js</strong>: Added retina canvas support (<code>devicePixelRatio</code>-aware sizing). Dots are now crisp on HiDPI displays.</p>
<p><strong>typewriter.js</strong>: Replaced inline setTimeout debounce with shared <code>debounce()</code> utility from app.js.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span><span class="ev-label" style="--label-color:var(--apple-pink)">UI/UX</span></div><h3>02:00 — Expert Team: CSS Performance Overhaul <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Applied the 6-expert system (FPS, Smooth Animator, Logic & Resilience, Tech Stack, Clean Code, QA) to the entire Show codebase, starting with the foundation: <code>style.css</code> and <code>app.js</code>.</p></div><div class="ev-body"><p>Applied the 6-expert system (FPS, Smooth Animator, Logic & Resilience, Tech Stack, Clean Code, QA) to the entire Show codebase, starting with the foundation: <code>style.css</code> and <code>app.js</code>.</p>
<p><strong>CSS</strong>: Removed <code>backdrop-filter: blur(12px)</code> from <code>.glass-tag</code> — this was the single worst performance killer, creating 100+ GPU blur passes on grid views. Replaced all 14 instances of <code>transition: all</code> with specific properties (<code>border-color, background, color, transform, opacity</code>). Added Apple HIG motion grammar (<code>--ease-out-expo</code>, <code>--ease-out-quart</code>, <code>--ease-spring</code>) and timing variables (<code>--duration-fast/normal/slow</code>). Added <code>content-visibility: hidden</code> for inactive views. Added <code>@media (prefers-reduced-motion: reduce)</code>. Added <code>will-change: transform</code> on animated elements.</p>
<p><strong>app.js</strong>: Added <code>fetchJSON()</code> with HTTP status validation. Timer management (<code>registerTimer()</code>, <code>clearAllTimers()</code>) called on every view switch to prevent interval/rAF leaks. Progressive lightbox loading (micro → display). <code>hashchange</code> listener for browser back/forward. Error boundaries with try/catch around experience init. Scroll-to-top on view switch. Extracted constants.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span></div><h3>00:55 — Signal Status: 14/20 Stages Complete <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Running processes finally producing real output. Gemini analysis at 70% (6,294/9,011). OCR running at 47% (4,223/9,011). All other signals at 100%. Emotions was already done — the 19% was misleading because only 1,676 images have faces, and all 1,676 have emotions.</p></div><div class="ev-body"><p>Running processes finally producing real output. Gemini analysis at 70% (6,294/9,011). OCR running at 47% (4,223/9,011). All other signals at 100%. Emotions was already done — the 19% was misleading because only 1,676 images have faces, and all 1,676 have emotions.</p>
<p>Updated State dashboard with accurate numbers across the board. Signal Inventory table now shows BLIP Captions DONE (9,011), Facial Emotions DONE (1,676 images with faces), Gemini at 6,294, OCR at 4,223. Architecture section updated from "9 Python Scripts" to 10 with the new orchestrator. Next priorities: finish Gemini + OCR, then GCS upload pipeline.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span></div><h3>00:50 — Master Orchestrator: mad_completions.py <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>The project had a recurring problem: analysis processes would hang, die silently, or never start. The user would wake up to find nothing completed. No more.</p></div><div class="ev-body"><p>The project had a recurring problem: analysis processes would hang, die silently, or never start. The user would wake up to find nothing completed. No more.</p>
<p>Built <code>mad_completions.py</code> — a master orchestrator that checks all 20 pipeline stages against the database: infrastructure (rendering, EXIF, colors, hashes), models (11 CV models + Gemini + vectors), enhancement, AI variants, and GCS uploads. For any gap found, it identifies the correct fix script and starts it with proper <code>PYTHONUNBUFFERED=1</code> logging. Resource-aware: only one GPU-heavy process at a time, API processes run alongside. <code>--watch</code> mode loops until everything reaches 100%. After each cycle, regenerates the State dashboard so it always reflects reality. Previously, the Gemini engine had been silently stuck for 8 hours because it was started with <code>--limit</code> instead of <code>--test</code>. The orchestrator makes manual process babysitting obsolete.</p></div></div>
<h2 class="date-header">2026-02-06</h2>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-pink)">UI/UX</span></div><h3>23:15 — Sidebar: Drift → Similarity <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> User still saw "Drift" in the sidebar navigation. The experiments section in <code>page_shell()</code> hadn't been renamed.</p></div><div class="ev-body"><p><strong>Intent.</strong> User still saw "Drift" in the sidebar navigation. The experiments section in <code>page_shell()</code> hadn't been renamed.</p>
<p>Renamed the sidebar link from "Drift" to "Similarity" in <code>page_shell()</code>. The web gallery already had both La Similarité (semantic) and La Dérive (structural) as separate, correctly named experiences.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>23:19 — Similarity: Interactive Vector Explorer <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> The old Drift page was a static dump of 10 random images with their neighbors — no interaction, no navigation. The user described an interactive experience twice. Now it's real: a dynamic similarity explorer with a live API.</p></div><div class="ev-body"><p><strong>Intent.</strong> The old Drift page was a static dump of 10 random images with their neighbors — no interaction, no navigation. The user described an interactive experience twice. Now it's real: a dynamic similarity explorer with a live API.</p>
<p>Rewrote <code>render_drift()</code> into an interactive single-page app. Start with a random image shown large. Below it: 3 model sections (DINOv2/SigLIP/CLIP) each with an 8-neighbor grid. Click any neighbor to navigate there — it becomes the new query. Breadcrumb trail tracks your journey. Back button. Random button. New API endpoints: <code>/api/similarity/<uuid></code> returns 8 nearest neighbors per model, <code>/api/similarity/random</code> picks a random starting image. Lazy lancedb connection shared across requests.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>23:15 — README Gets Card Layout <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> README page was plain rendered markdown while System Instructions had beautiful card-based layout with colored accent borders and pill labels. They should match.</p></div><div class="ev-body"><p><strong>Intent.</strong> README page was plain rendered markdown while System Instructions had beautiful card-based layout with colored accent borders and pill labels. They should match.</p>
<p>Rewrote <code>render_readme()</code> from a generic markdown-to-HTML converter into a section-aware card renderer. Each ## section becomes an <code>inst-card</code> with a colored pill label (Hardware/orange, Creative/pink, Architecture/blue, Infrastructure/teal). Three Apps section renders as <code>app-trio</code> boxes. Tables, lists, and ordered lists all render correctly inside cards.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-pink)">UI/UX</span><span class="ev-label" style="--label-color:var(--apple-indigo)">Architecture</span></div><h3>23:10 — Three Apps: Intent Over State <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> The Three Apps descriptions (Show, State, See) read like a feature list. They should express intent and purpose. Show exists to blow minds and delight. State is the control room. See is the native power viewer where the human eye decides.</p></div><div class="ev-body"><p><strong>Intent.</strong> The Three Apps descriptions (Show, State, See) read like a feature list. They should express intent and purpose. Show exists to blow minds and delight. State is the control room. See is the native power viewer where the human eye decides.</p>
<p>Rewrote all Three Apps descriptions in the briefing (System Instructions), Genesis event (Journal), and README.md. Removed "Then human curation. Then a public gallery that only shows the accepted best" — the apps speak for themselves now. Show leads the trio: "Blow people's minds. Continuously release new experiences guided by signals and new ideas. Delightful, playful, elegant, smart, teasing, revealing, exciting — on every screen."</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>01:20 — State: Accurate Signal Inventory + Sidebar Fix + "As of" Timestamp <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> State dashboard showed stale numbers ("16/16 signals, 6,203 Gemini"). The signal inventory claimed everything was done when only 12/18 signals were complete. Sidebar items shifted on click due to border-left appearing.</p></div><div class="ev-body"><p><strong>Intent.</strong> State dashboard showed stale numbers ("16/16 signals, 6,203 Gemini"). The signal inventory claimed everything was done when only 12/18 signals were complete. Sidebar items shifted on click due to border-left appearing.</p>
<p>Updated <code>render_instructions()</code> with accurate counts for all 18 signals (green checkmarks for complete, live numbers for in-progress). Fixed sidebar shift by giving all links a transparent 3px left border at baseline. Added "As of [date]" timestamp in hero subtitle for static deployments. Deployed to GitHub Pages and Firebase.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span><span class="ev-label" style="--label-color:var(--apple-pink)">UI/UX</span></div><h3>01:15 — La Dérive: Real DINOv2 Visual Drift <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> Transform La Dérive from metadata-based heuristics into real visual embedding similarity. The user wants incredible pairs: completely different images that share abstract visual structure — a bridge and a ribcage, a shoe and a ramp. DINOv2 captures texture and structure, not content.</p></div><div class="ev-body"><p><strong>Intent.</strong> Transform La Dérive from metadata-based heuristics into real visual embedding similarity. The user wants incredible pairs: completely different images that share abstract visual structure — a bridge and a ribcage, a shoe and a ramp. DINOv2 captures texture and structure, not content.</p>
<p>Precomputed 8 nearest DINOv2 neighbors for all 9,011 images (768d vectors, cosine similarity). Exported to <code>drift_neighbors.json</code> (5.2MB). Rewrote <code>drift.js</code> to load and navigate these embedding-based neighbors. Added <code>loadDriftNeighbors()</code> to <code>app.js</code>. Added subtle similarity score bar to neighbor cards.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>01:00 — Emotions Bug: Normalized Coordinates Were Producing 1×1 Pixel Crops <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> The facial emotions process completed 2,545 face classifications, but investigation revealed ALL of them were garbage. Face detection stores coordinates as normalized values (0-1 range), but the emotion code treated them as pixel coordinates — producing 0×0 or 1×1 pixel crops fed to the ViT classifier. Every emotion label was nonsense.</p></div><div class="ev-body"><p><strong>Intent.</strong> The facial emotions process completed 2,545 face classifications, but investigation revealed ALL of them were garbage. Face detection stores coordinates as normalized values (0-1 range), but the emotion code treated them as pixel coordinates — producing 0×0 or 1×1 pixel crops fed to the ViT classifier. Every emotion label was nonsense.</p>
<p>Fixed <code>advanced_signals.py</code> to multiply normalized coordinates by image dimensions before cropping. Added minimum crop size check (10px). Moved try/except to per-face level so one bad face doesn't skip the whole image. Deleted all bad emotion data. Re-running with --force, but OCR shards are locking the DB. Will retry once OCR finishes.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>00:30 — Signals Progressing Overnight <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> Check-in on all background analysis processes running since the previous session.</p></div><div class="ev-body"><p><strong>Intent.</strong> Check-in on all background analysis processes running since the previous session.</p>
<p>Five processes still alive: 3 OCR shards (28%, 2,543/9,011), photography_engine for Gemini (68.9%, 6,210/9,011), facial emotions (79.7%, 2,541/3,187 faces). Face detections jumped from 1,676 to 3,187 — more faces discovered as analysis expanded. Emotions climbed from 1,367 to 2,541. BLIP captions stuck at 9,006/9,011 — 5 images blocked by SQLite locks from concurrent OCR shards. Will retry once OCR finishes.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-pink)">UI/UX</span></div><h3>23:00 — Landing Page: Bold Mission + Game is ON <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> Mission statement needed to stand out. Changed "on different screens" to "on screens". Added "GAME IS ON." tagline.</p></div><div class="ev-body"><p><strong>Intent.</strong> Mission statement needed to stand out. Changed "on different screens" to "on screens". Added "GAME IS ON." tagline.</p>
<p>Mission text now bold black (weight 700) at base font size. Added uppercase "GAME IS ON." below in muted gray with caps tracking. Deployed to GitHub Pages.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-pink)">UI/UX</span><span class="ev-label" style="--label-color:var(--apple-teal)">Signal</span></div><h3>22:50 — State Dashboard: Category-Themed Tags + Compact Journal <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> Tags in State dashboard all looked identical. User wanted category-specific color theming like in Show. Journal events were too long — needed compact default with click-to-expand.</p></div><div class="ev-body"><p><strong>Intent.</strong> Tags in State dashboard all looked identical. User wanted category-specific color theming like in Show. Journal events were too long — needed compact default with click-to-expand.</p>
<p>Added 7 category color classes to State tags: vibe=orange, grading=blue, time=gold, setting=green, exposure=teal, composition=purple, camera=silver. Updated <code>tags()</code> JS function to accept category parameter. Journal events now collapsed by default — show title + labels + key "why it matters" line. Click toggles full body.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>22:45 — State Instructions Page Restyled <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> User pointed out System Instructions page was completely outdated in style and content. Needed card-based layout, not a wall of text.</p></div><div class="ev-body"><p><strong>Intent.</strong> User pointed out System Instructions page was completely outdated in style and content. Needed card-based layout, not a wall of text.</p>
<p>Complete rewrite of <code>render_instructions()</code>: card-based layout with colored accent borders (indigo=briefing, pink=creative, green=status), 2-column grids for cameras and architecture, app trio boxes, category-themed signal inventory table. Added incremental ingestion pipeline card. Removed verbose Development Principles prose — replaced with compact actionable rules.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>22:30 — State Dashboard: Cleanup + Creative Direction + Self-Instructions <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> User flagged that State dashboard content was completely outdated. Project Briefing still said "3 experiences", Imagen Variants section was irrelevant, signal counts were stale.</p></div><div class="ev-body"><p><strong>Intent.</strong> User flagged that State dashboard content was completely outdated. Project Briefing still said "3 experiences", Imagen Variants section was irrelevant, signal counts were stale.</p>
<p>Removed Imagen Variant Generation section entirely (HTML, CSS, JavaScript). Updated Project Briefing: Show now lists all 14 experiences. Updated signal completion counts (Gemini 6,203/9,011, OCR complete, BLIP 8,933/9,011, Emotions 1,367/1,676). Updated "Done vs. Next" to reflect actual state. Added "Creative Direction for Show" section to instructions — signal-aware storytelling, emotional moments, minimalist UI.</p>
<p><strong>Self-instruction written:</strong> Added mandatory rule to MEMORY.md — always update <code>generate_status_page.py</code> instructions when architecture changes, just like the journal. Also added creative direction mandate: Show experiences must be designed by someone who is simultaneously developer, architect, ML engineer, Apple-level designer, and emotionally intelligent creative director. Pairing two laughing faces IS funny. A rose next to rose accents IS pretty.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>22:00 — Show: 14 Image Experiences Built <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> Transform the web gallery from 3 experiences into 14 extraordinary ways to explore 9,011 photographs. Every signal extracted by the pipeline should power a different kind of encounter with the images.</p></div><div class="ev-body"><p><strong>Intent.</strong> Transform the web gallery from 3 experiences into 14 extraordinary ways to explore 9,011 photographs. Every signal extracted by the pipeline should power a different kind of encounter with the images.</p>
<p>Complete rewrite of the web gallery architecture. Launcher page with 14 experience cards. New <code>export_gallery_data.py</code> exports ALL 9,011 images with 47 signal fields each (not just the 5,038 Gemini-analyzed). Four data files generated: <code>photos.json</code> (15.8MB), <code>faces.json</code> (315KB, 1,676 faces with emotions), <code>game_rounds.json</code> (49KB, 200 precomputed connection pairs), <code>stream_sequence.json</code> (336KB, palette-optimized viewing order).</p>
<p><strong>New experiences:</strong> Le Bento (Mondrian mosaic with chromatic harmony), La Similarité (renamed from drift — semantic neighbors with inverted-index matching), La Dérive (new creative structural drift using composition/depth/brightness), Le Terrain de Jeu (connection game with 8s timer and streak scoring), Le Flot (infinite curated stream with monochrome breathers), La Chambre Noire (toggleable signal layers: colors, depth, objects, faces, OCR, metadata), Les Visages (face wall with emotion filtering), La Boussole (4-axis compass navigation), L'Observatoire (6 data panels: cameras, aesthetics, time, styles, emotions, outliers), La Carte (GPS dots on dark canvas map), La Machine à Écrire (weighted text search across all fields), Le Pendule (aesthetic taste test).</p>
<p><strong>Design system:</strong> Category-colored tags (vibe=amber, grading=blue, time=golden, setting=green, scene=teal, emotion=pink, camera=silver, style=purple) with capitalized text, subtle borders, hover states. Applied across grid, lightbox, and all experiences.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>20:30 — System Instructions: Complete Project Briefing <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> When starting a new AI session, context is everything. Added a comprehensive "Project Briefing" section at the top of the System Instructions page — everything a new session needs to be immediately productive: the 5 cameras with their quirks and enhancement rules, all 9 scripts with purposes, critical technical rules (Python 3.9, Vertex AI only, flat layout, DNG color space), hard-won lessons (Monochrom is sacred, film grain is an asset, TF+PyTorch don't mix, LAION scores are useless), GCS bucket structure, MADCurator architecture, web gallery setup, journal format, and a done/in-progress/next status summary. Rendered as an indigo-bordered card at the top of <code>/instructions</code>.</p></div><div class="ev-body"><p><strong>Intent.</strong> When starting a new AI session, context is everything. Added a comprehensive "Project Briefing" section at the top of the System Instructions page — everything a new session needs to be immediately productive: the 5 cameras with their quirks and enhancement rules, all 9 scripts with purposes, critical technical rules (Python 3.9, Vertex AI only, flat layout, DNG color space), hard-won lessons (Monochrom is sacred, film grain is an asset, TF+PyTorch don't mix, LAION scores are useless), GCS bucket structure, MADCurator architecture, web gallery setup, journal format, and a done/in-progress/next status summary. Rendered as an indigo-bordered card at the top of <code>/instructions</code>.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span></div><h3>20:15 — Journal de Bord: Full Content + Event Type Labels + Genesis <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> The journal renderer was truncating all event content to first sentences and dropping paragraphs entirely. User wanted full event details as a beautiful stream with categorized event labels.</p></div><div class="ev-body"><p><strong>Intent.</strong> The journal renderer was truncating all event content to first sentences and dropping paragraphs entirely. User wanted full event details as a beautiful stream with categorized event labels.</p>
<p><strong>What changed.</strong> Complete rewrite of <code>render_journal()</code> in <code>generate_status_page.py</code>:</p>
<ul>
<li><strong>Full content</strong>: Removed <code>first_sentence()</code> truncation and <code>skip_rest</code> logic. All paragraphs, blockquotes, lists, tables, and code blocks now render completely.</li>
<li><strong>Event type labels</strong>: Auto-classification system with 9 categories (Deploy, Infrastructure, Pipeline, AI, Investigation, UI/UX, Security, Architecture, Signal) using regex pattern matching on title + body. Each event gets up to 2 colored pill labels using <code>color-mix()</code> for subtle tinted backgrounds.</li>
<li><strong>Removed intro sections</strong>: "The Beginning" and "The Numbers" prose blocks no longer appear in the Journal de Bord — they were redundant with the timeline.</li>
<li><strong>Genesis event</strong>: Special indigo-bordered card at the bottom of the timeline summarizing the project vision: the 3 apps (See/Show/State), the mission, the endgame.</li>
<li><strong>Rich formatting</strong>: Tables render properly, code fences get <code><pre><code></code> blocks, <strong>Solution.</strong> and other bold-prefixed paragraphs get distinct styling, all markdown inline formatting (bold, italic, code) preserved.</li>
</ul></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span><span class="ev-label" style="--label-color:var(--apple-pink)">UI/UX</span></div><h3>20:00 — State UI: Mosaic Hero + Compact Model Cards <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> User rejected the horizontal filmstrip ("why is there a row of images?"). Wanted a mosaic on the right side of the title area and more compact Signal Extraction cards.</p></div><div class="ev-body"><p><strong>Intent.</strong> User rejected the horizontal filmstrip ("why is there a row of images?"). Wanted a mosaic on the right side of the title area and more compact Signal Extraction cards.</p>
<p><strong>What changed.</strong> Replaced the filmstrip with a mosaic-on-right hero layout: <code>.state-hero</code> flex container with text on left and a 280px rounded mosaic image on right, fading in with cubic-bezier animation on load. The 17-element intelligence grid cards were compacted dramatically: grid cells from 200px to 160px minimum, padding reduced to 8px, model descriptions and percentage labels hidden, font sizes shrunk, progress bars to 2px height. The result is a dense overview where all 17 models fit on screen without scrolling.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>19:50 — State UI: GCS Filmstrip + Preload Animations <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Added a horizontal filmstrip of 40 randomly sampled photographs below the manifesto on the State page. Images load from GCS (<code>v/original/thumb/jpeg/</code>) with a cubic-bezier fade-in animation — each image starts at <code>opacity: 0; scale: 1.08</code> and smoothly transitions to <code>opacity: 1; scale: 1</code> on load. Same treatment applied to drift page neighbor thumbnails. Removed all local image serving handlers — everything now served from GCS. No more <code>/thumb/</code> or <code>/blind/</code> local routes.</p></div><div class="ev-body"><p>Added a horizontal filmstrip of 40 randomly sampled photographs below the manifesto on the State page. Images load from GCS (<code>v/original/thumb/jpeg/</code>) with a cubic-bezier fade-in animation — each image starts at <code>opacity: 0; scale: 1.08</code> and smoothly transitions to <code>opacity: 1; scale: 1</code> on load. Same treatment applied to drift page neighbor thumbnails. Removed all local image serving handlers — everything now served from GCS. No more <code>/thumb/</code> or <code>/blind/</code> local routes.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span></div><h3>19:48 — GCS Upload: Originals Complete, Enhanced In Progress <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>All original serving tiers successfully uploaded to GCS: display, mobile, thumb, micro in both JPEG and WebP — 8 directories, ~72K files total. Each directory got immutable cache headers (<code>max-age=31536000</code>). Enhanced v1 tiers uploading next — same 8 directories. Public URLs verified working: <code>https://storage.googleapis.com/myproject-public-assets/art/MADphotos/v/original/{tier}/{format}/{uuid}.ext</code>.</p></div><div class="ev-body"><p>All original serving tiers successfully uploaded to GCS: display, mobile, thumb, micro in both JPEG and WebP — 8 directories, ~72K files total. Each directory got immutable cache headers (<code>max-age=31536000</code>). Enhanced v1 tiers uploading next — same 8 directories. Public URLs verified working: <code>https://storage.googleapis.com/myproject-public-assets/art/MADphotos/v/original/{tier}/{format}/{uuid}.ext</code>.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-pink)">UI/UX</span></div><h3>19:44 — Enhanced v1 Tier Rendering Complete: Zero Errors <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>All 9,011 enhanced v1 images rendered into 7 tier/format combinations: display/webp, mobile/jpeg, mobile/webp, thumb/jpeg, thumb/webp, micro/jpeg, micro/webp. Zero errors across the entire batch. The <code>render_enhanced_tiers.py</code> script processed everything using 8 parallel workers, downscaling from the existing 2048px display-tier JPEGs with appropriate sharpening per tier.</p></div><div class="ev-body"><p>All 9,011 enhanced v1 images rendered into 7 tier/format combinations: display/webp, mobile/jpeg, mobile/webp, thumb/jpeg, thumb/webp, micro/jpeg, micro/webp. Zero errors across the entire batch. The <code>render_enhanced_tiers.py</code> script processed everything using 8 parallel workers, downscaling from the existing 2048px display-tier JPEGs with appropriate sharpening per tier.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>19:35 — Blind Test Verdict: Enhanced v1 and v2 Are Nearly Identical <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Investigation confirmed the user's observation: enhanced v1 and v2 differ by a mean of only 0.50 pixels (max 12). The v2 enhancement (signal-aware) adds subtle depth, scene, and style corrections on top of v1's base camera-aware processing — but the perceptual difference is negligible. For the "Show" web app, we'll focus on enhanced v1 as the primary improved version. All enhancement parameters are fully saved in <code>enhancement_plans</code> and <code>enhancement_plans_v2</code> tables for future recipe tuning.</p></div><div class="ev-body"><p>Investigation confirmed the user's observation: enhanced v1 and v2 differ by a mean of only 0.50 pixels (max 12). The v2 enhancement (signal-aware) adds subtle depth, scene, and style corrections on top of v1's base camera-aware processing — but the perceptual difference is negligible. For the "Show" web app, we'll focus on enhanced v1 as the primary improved version. All enhancement parameters are fully saved in <code>enhancement_plans</code> and <code>enhancement_plans_v2</code> tables for future recipe tuning.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>19:34 — Enhanced v1 Tier Rendering + GCS Upload Begins <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Started rendering the enhanced v1 tier pyramid — the enhanced images existed only as 2048px JPEGs. Now generating mobile (1280px), thumb (480px), micro (64px) in JPEG + WebP for all 9,011 images. Original serving tiers (thumb JPEG/WebP: 132MB) already uploaded to GCS. Blind test images (300 files, 169MB) uploaded to <code>v/blind/</code> on GCS. Static pages updated to reference GCS URLs directly — no more local <code>docs/blind/</code> directory (saved 169MB from the repo).</p></div><div class="ev-body"><p>Started rendering the enhanced v1 tier pyramid — the enhanced images existed only as 2048px JPEGs. Now generating mobile (1280px), thumb (480px), micro (64px) in JPEG + WebP for all 9,011 images. Original serving tiers (thumb JPEG/WebP: 132MB) already uploaded to GCS. Blind test images (300 files, 169MB) uploaded to <code>v/blind/</code> on GCS. Static pages updated to reference GCS URLs directly — no more local <code>docs/blind/</code> directory (saved 169MB from the repo).</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span></div><h3>19:30 — GCS Bucket Architecture: Versioned Image Hosting <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Designed and implemented a clean versioned structure for the GCS bucket. All images now live under <code>v/{version}/{tier}/{format}/{uuid}.ext</code>. The "version" dimension covers: <code>original</code> (base photographs), <code>enhanced</code> (camera-aware enhancement v1), <code>enhanced_v2</code> (signal-aware enhancement), and future AI variants. Each version has its own tier pyramid (display, mobile, thumb, micro). URL pattern is fully programmatic — any web app can construct image URLs from just a UUID and version name. Rewrote <code>gcs_sync.py</code> completely to support the new layout with <code>--version</code> and <code>--tiers</code> flags.</p></div><div class="ev-body"><p>Designed and implemented a clean versioned structure for the GCS bucket. All images now live under <code>v/{version}/{tier}/{format}/{uuid}.ext</code>. The "version" dimension covers: <code>original</code> (base photographs), <code>enhanced</code> (camera-aware enhancement v1), <code>enhanced_v2</code> (signal-aware enhancement), and future AI variants. Each version has its own tier pyramid (display, mobile, thumb, micro). URL pattern is fully programmatic — any web app can construct image URLs from just a UUID and version name. Rewrote <code>gcs_sync.py</code> completely to support the new layout with <code>--version</code> and <code>--tiers</code> flags.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-pink)">UI/UX</span></div><h3>19:18 — Static Site: All 6 Pages Generated <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Ran <code>generate_static()</code> — successfully output all 6 pages to <code>docs/</code>: state.html (75KB), journal.html (36KB), instructions.html (18KB), drift.html (44KB), blind-test.html (112KB), mosaics.html (24KB). All sidebar links properly rewritten from server routes (<code>/journal</code>) to static paths (<code>journal.html</code>). Every page has the collapsible sidebar, theme toggle, and hamburger menu.</p></div><div class="ev-body"><p>Ran <code>generate_static()</code> — successfully output all 6 pages to <code>docs/</code>: state.html (75KB), journal.html (36KB), instructions.html (18KB), drift.html (44KB), blind-test.html (112KB), mosaics.html (24KB). All sidebar links properly rewritten from server routes (<code>/journal</code>) to static paths (<code>journal.html</code>). Every page has the collapsible sidebar, theme toggle, and hamburger menu.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>19:18 — Dashboard Cards: Element Table Redesign <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Replaced the 8 plain white stat cards with a dramatic two-section layout inspired by periodic table element cards:</p></div><div class="ev-body"><p>Replaced the 8 plain white stat cards with a dramatic two-section layout inspired by periodic table element cards:</p>
<p><strong>3 Hero Cards</strong> at the top — bold gradient backgrounds (blue, green, purple) with white text, showing Collection (9,011 photographs), Intelligence (total signals extracted across all models), and Output (rendered files + enhanced + AI variants).</p>
<p><strong>17-Element Intelligence Grid</strong> below — each model gets its own tinted card with a unique hue-based color scheme (HSL custom properties), showing: model name, description, image count, a mini progress bar, and a status badge (complete/in-progress/pending). All 17 models listed: Gemini 2.5 Pro, Pixel Analysis, DINOv2, SigLIP, CLIP, YuNet, YOLOv8n, NIMA, Depth Anything v2, Places365, Style Net, BLIP, EasyOCR, Emotions, Enhancement Engine, K-means LAB, EXIF Parser. Each card's description includes live stats like "3,247 faces found" or "avg 4.8 aesthetic score."</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-orange)">Investigation</span></div><h3>19:12 — Full Sidebar Sync + Collapse Toggle <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Unified the sidebar across all 7 pages: README, State, Journal, Instructions, Drift, Blind Test, Mosaics. All pages now share the identical sidebar structure with the same links. Added a collapsible sidebar system: on desktop, a "Hide sidebar" button at the bottom collapses the sidebar to zero width with a smooth CSS transition; a floating hamburger button appears at the top-left to bring it back. State persisted in <code>localStorage</code> so it survives page navigation. On mobile (<900px), the collapse button is hidden — mobile uses the existing hamburger/top-bar pattern instead.</p></div><div class="ev-body"><p>Unified the sidebar across all 7 pages: README, State, Journal, Instructions, Drift, Blind Test, Mosaics. All pages now share the identical sidebar structure with the same links. Added a collapsible sidebar system: on desktop, a "Hide sidebar" button at the bottom collapses the sidebar to zero width with a smooth CSS transition; a floating hamburger button appears at the top-left to bring it back. State persisted in <code>localStorage</code> so it survives page navigation. On mobile (<900px), the collapse button is hidden — mobile uses the existing hamburger/top-bar pattern instead.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-pink)">UI/UX</span></div><h3>19:10 — Landing Page: Mosaic Floats Right <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Rewrote the landing page hero entirely. Instead of a full-width dark overlay image, the mosaic is now a beautiful rounded rectangle (<code>border-radius: 20px</code>, <code>box-shadow: var(--shadow-lg)</code>) floating to the right of the title and subtitle text on desktop, taking the height of the text content. On mobile (<700px), it stacks on top as a wide banner. The title "9,011" + "photographs, unedited" sits left, the rounded mosaic card sits right. Clean Apple layout — text breathes, image is decorative not dominant.</p></div><div class="ev-body"><p>Rewrote the landing page hero entirely. Instead of a full-width dark overlay image, the mosaic is now a beautiful rounded rectangle (<code>border-radius: 20px</code>, <code>box-shadow: var(--shadow-lg)</code>) floating to the right of the title and subtitle text on desktop, taking the height of the text content. On mobile (<700px), it stacks on top as a wide banner. The title "9,011" + "photographs, unedited" sits left, the rounded mosaic card sits right. Clean Apple layout — text breathes, image is decorative not dominant.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span></div><h3>17:55 — Dashboard: Card Redesign + Mobile Responsive <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Removed the redundant Gemini Analysis progress section — that data was duplicated in the top cards. Redesigned top stat cards: replaced "Gemini AI", "Pixel Analysis", "GCS Uploads" with "AI Models Active" (shows X/10 models complete), "Enhanced" (enhancement plans with %), "Faces Found" (total faces + emotion count), "Vector Embeddings" (count × 3 models). Cards sorted by activity/interest.</p></div><div class="ev-body"><p>Removed the redundant Gemini Analysis progress section — that data was duplicated in the top cards. Redesigned top stat cards: replaced "Gemini AI", "Pixel Analysis", "GCS Uploads" with "AI Models Active" (shows X/10 models complete), "Enhanced" (enhancement plans with %), "Faces Found" (total faces + emotion count), "Vector Embeddings" (count × 3 models). Cards sorted by activity/interest.</p>
<p>Mobile CSS completely reworked: hamburger menu for sidebar at <900px with backdrop blur, sticky top bar, cards always 2-column on mobile, tables scroll horizontally, hero collapses gracefully (hides tagline/mission on small screens). Media queries switched from max-width to min-width (mobile-first).</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>17:45 — The Landing Page: Magazine-Quality README <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Created a gorgeous dark-themed landing page for GitHub Pages (<code>docs/index.html</code>). Full-viewport hero with the brightness-sorted mosaic, giant "9,011" counter with gradient text, the mission statement, and a smooth-scroll "Explore" button. Below: navigation cards (State, Journal de Bord, GitHub) with colored accent borders, the camera collection list, three apps section (See/Show/State), the 9-stage pipeline with numbered step indicators, infrastructure grid, and 10 model pills. All mobile-first: base CSS is for phones, <code>min-width</code> media queries at 640px, 960px, 1200px. Pure dark (#0A0A0A) with Apple SF Pro typography.</p></div><div class="ev-body"><p>Created a gorgeous dark-themed landing page for GitHub Pages (<code>docs/index.html</code>). Full-viewport hero with the brightness-sorted mosaic, giant "9,011" counter with gradient text, the mission statement, and a smooth-scroll "Explore" button. Below: navigation cards (State, Journal de Bord, GitHub) with colored accent borders, the camera collection list, three apps section (See/Show/State), the 9-stage pipeline with numbered step indicators, infrastructure grid, and 10 model pills. All mobile-first: base CSS is for phones, <code>min-width</code> media queries at 640px, 960px, 1200px. Pure dark (#0A0A0A) with Apple SF Pro typography.</p>
<p>Dashboard moved to <code>docs/state.html</code>. Added <code>.nojekyll</code> to bypass Jekyll processing. Sidebar links updated for static file routing.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>17:30 — OCR Sharding: 3x Parallel Workers <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>OCR was crawling at 0.2/s — 12+ hour ETA for 8,000 remaining images. Added <code>--shard N/M</code> argument to <code>advanced_signals.py</code> that partitions work by <code>hash(uuid) % M == N</code>. Killed the single OCR process and launched 3 parallel workers: shard 0/3 (2,684 images), shard 1/3 (2,733), shard 2/3 (2,671). Each runs its own EasyOCR reader on CPU. Combined throughput should be ~0.6/s, bringing ETA down to ~4 hours. Emotions already at 1,367/1,676 — almost done on its own.</p></div><div class="ev-body"><p>OCR was crawling at 0.2/s — 12+ hour ETA for 8,000 remaining images. Added <code>--shard N/M</code> argument to <code>advanced_signals.py</code> that partitions work by <code>hash(uuid) % M == N</code>. Killed the single OCR process and launched 3 parallel workers: shard 0/3 (2,684 images), shard 1/3 (2,733), shard 2/3 (2,671). Each runs its own EasyOCR reader on CPU. Combined throughput should be ~0.6/s, bringing ETA down to ~4 hours. Emotions already at 1,367/1,676 — almost done on its own.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>18:40 — Git Push: The Big One <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>37 files, 16,564 insertions. The full pipeline code, all 3 apps (See/Show/State), 16 analysis signals, dual enhancement engines, blind test system, web gallery, GitHub Pages deployment workflow. OCR restarted after the v2 enhancement freed up DB locks.</p></div><div class="ev-body"><p>37 files, 16,564 insertions. The full pipeline code, all 3 apps (See/Show/State), 16 analysis signals, dual enhancement engines, blind test system, web gallery, GitHub Pages deployment workflow. OCR restarted after the v2 enhancement freed up DB locks.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-pink)">UI/UX</span></div><h3>18:35 — Hero Landing on State Dashboard <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Added a hero section to the State dashboard. Full-bleed brightness-sorted mosaic (9,011 tiny images) as background with dark gradient overlay. Title: "MADphotos / 9,011 photographs". Mission statement explains the per-image intelligence philosophy. Responsive down to 440px. Hero mosaic resized to 1200px (513KB) for GitHub Pages. Pushed and deployed.</p></div><div class="ev-body"><p>Added a hero section to the State dashboard. Full-bleed brightness-sorted mosaic (9,011 tiny images) as background with dark gradient overlay. Title: "MADphotos / 9,011 photographs". Mission statement explains the per-image intelligence philosophy. Responsive down to 440px. Hero mosaic resized to 1200px (513KB) for GitHub Pages. Pushed and deployed.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>18:30 — V2 Enhancement Complete: 9,276 Images, Zero Errors <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Enhancement Engine V2 completed in 427.7 seconds (21.7 images/s). All 9,276 images processed with zero errors. The signal-aware recipes work — each image now has a second enhanced version that was computed using depth, scene, style, Gemini vibes, and face detection data. Output lives in <code>rendered/enhanced_v2/jpeg/</code>.</p></div><div class="ev-body"><p>Enhancement Engine V2 completed in 427.7 seconds (21.7 images/s). All 9,276 images processed with zero errors. The signal-aware recipes work — each image now has a second enhanced version that was computed using depth, scene, style, Gemini vibes, and face detection data. Output lives in <code>rendered/enhanced_v2/jpeg/</code>.</p>
<p>Blind test generated: 100 diverse images sampled across all 6 cameras (41 M8, 33 Osmo Pro, 12 Monochrom, 12 MP, 1 G12, 1 Memo). Each row has 3 versions (original, v1, v2) in random order — all 6 permutations represented. The moment of truth: http://localhost:8080/blind-test</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>18:15 — Blind Test Redesign: True 3-Way Comparison <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Rewrote the blind test page for a proper A/B/C comparison. New design:</p></div><div class="ev-body"><p>Rewrote the blind test page for a proper A/B/C comparison. New design:</p>
<ul>
<li>100 rows, each with Original + Enhanced v1 + Enhanced v2 in <strong>random order per row</strong></li>
<li>No labels until reveal — images marked only A, B, C</li>
<li>Selected image elevates with shadow and blue border (translateY -4px, 24px box-shadow)</li>
<li>Live scoreboard showing picks vs. remaining</li>
<li>Reveal shows color-coded horizontal bar chart (Original=gray, V1=blue, V2=green)</li>
<li><code>prep_blind_test.py</code> script handles diverse sampling across cameras and styles</li>
</ul>
<p>Stopped OCR process temporarily (12+ hours ETA, 0.2/s) to reduce DB contention — was causing lock failures across all processes. Captions and emotions continue running.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>18:00 — Enhancement Engine V2: Signal-Aware Processing <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Built <code>enhance_engine_v2.py</code> — a new enhancement engine that uses ALL available signals to make per-image editing decisions. Beyond the v1 camera-aware pixel metrics, v2 incorporates:</p></div><div class="ev-body"><p>Built <code>enhance_engine_v2.py</code> — a new enhancement engine that uses ALL available signals to make per-image editing decisions. Beyond the v1 camera-aware pixel metrics, v2 incorporates:</p>
<ul>
<li><strong>Depth estimation</strong> — foreground-dominant scenes get sharper contrast, landscapes get atmospheric protection</li>
<li><strong>Scene classification</strong> — warm interiors get warmer WB, nature scenes get saturation boost, dark scenes get shadow lift</li>
<li><strong>Style classification</strong> — street photography gets higher contrast + desaturation, portraits get softer processing</li>
<li><strong>Gemini vibes</strong> — moody images stay darker with more contrast, vibrant images get saturation boost, golden hour gets warmth</li>
<li><strong>Face detection</strong> — images with faces get more conservative exposure correction and gentler sharpening</li>
</ul>
<p>The engine reads all signals via LEFT JOINs (works even without all signals), computes a layered recipe where each signal modulates the base camera profile, and outputs to <code>rendered/enhanced_v2/jpeg/</code>. Includes <code>PRAGMA busy_timeout=120000</code> and 10-retry loops for SQLite contention.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-pink)">UI/UX</span></div><h3>17:45 — Apple.com-Style README & System Instructions Update <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>README page redesigned with apple.com-inspired cards: max-width 640px, generous whitespace, styled tables, refined typography. System Instructions page updated with current signal inventory — 16 signals total (9 complete, 3 in progress, 4 not started). Removed "Future Signals" section that was outdated — most of those are now running.</p></div><div class="ev-body"><p>README page redesigned with apple.com-inspired cards: max-width 640px, generous whitespace, styled tables, refined typography. System Instructions page updated with current signal inventory — 16 signals total (9 complete, 3 in progress, 4 not started). Removed "Future Signals" section that was outdated — most of those are now running.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span><span class="ev-label" style="--label-color:var(--apple-pink)">UI/UX</span></div><h3>17:30 — SVG Icon System for Tags <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>User feedback: emojis too noisy. Replaced all emoji tags with a custom inline SVG icon system — 16 hand-picked icons (camera, scene, depth, pin, palette, sun, star, sunset, bulb, frame, sparkle, rotate, film, box, eye, home). Each tag calls <code>tags(data, containerId, iconKey)</code> which looks up the SVG from the <code>IC</code> map. Clean, colored, consistent. Taller padding for breathing room.</p></div><div class="ev-body"><p>User feedback: emojis too noisy. Replaced all emoji tags with a custom inline SVG icon system — 16 hand-picked icons (camera, scene, depth, pin, palette, sun, star, sunset, bulb, frame, sparkle, rotate, film, box, eye, home). Each tag calls <code>tags(data, containerId, iconKey)</code> which looks up the SVG from the <code>IC</code> map. Clean, colored, consistent. Taller padding for breathing room.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-orange)">Investigation</span></div><h3>17:10 — Dashboard UI Overhaul: Tags, Sidebar, README <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Replaced all ugly icon-squares on tags with emojis — scenes get landscapes, cameras get camera emoji, locations get pins, vibes get sparkles, etc. Reduced tag border-radius from full pill to 6px. Added more padding for breathing room. Enhancement section now shows camera body breakdown (Leica M8: 3,533, DJI Osmo Pro: 3,032, etc.) instead of just "enhanced: 9,011".</p></div><div class="ev-body"><p>Replaced all ugly icon-squares on tags with emojis — scenes get landscapes, cameras get camera emoji, locations get pins, vibes get sparkles, etc. Reduced tag border-radius from full pill to 6px. Added more padding for breathing room. Enhancement section now shows camera body breakdown (Leica M8: 3,533, DJI Osmo Pro: 3,032, etc.) instead of just "enhanced: 9,011".</p>
<p>Restructured sidebar: removed "Pages" header, put README on top, renamed main dashboard to "State". Grouped Drift, Blind Test, and Mosaics under collapsible "Experiments" section. Dashboard section anchors now toggle open/closed.</p>
<p>Rewrote README with project vision: the 3 apps (See/Show/State), the intent, the full pipeline. Fixed image count everywhere from 11,557 to 9,011 (the actual count in originals/).</p>
<p>Journal de Bord entries now render as Twitter/X-style cards with borders, rounded corners, hover effects, and thread connector lines between entries.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>16:45 — TensorFlow Broke Everything <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Installing TensorFlow 2.20 + Keras 3.10 for DeepFace caused a C++ mutex crash in PyTorch/transformers. BLIP couldn't even load on CPU — <code>libc++abi: terminating due to uncaught exception of type std::__1::system_error: mutex lock failed</code>. Solution: uninstall TensorFlow entirely, rewrite the emotions phase to use <code>trpakov/vit-face-expression</code> (a ViT model that runs on PyTorch). Fixed column name mismatch in face_detections (<code>w</code>/<code>h</code> not <code>width</code>/<code>height</code>). Added SQLite retry logic with exponential backoff for concurrent write locks. All 3 models now running in parallel successfully.</p></div><div class="ev-body"><p>Installing TensorFlow 2.20 + Keras 3.10 for DeepFace caused a C++ mutex crash in PyTorch/transformers. BLIP couldn't even load on CPU — <code>libc++abi: terminating due to uncaught exception of type std::__1::system_error: mutex lock failed</code>. Solution: uninstall TensorFlow entirely, rewrite the emotions phase to use <code>trpakov/vit-face-expression</code> (a ViT model that runs on PyTorch). Fixed column name mismatch in face_detections (<code>w</code>/<code>h</code> not <code>width</code>/<code>height</code>). Added SQLite retry logic with exponential backoff for concurrent write locks. All 3 models now running in parallel successfully.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>16:18 — Running 3 Missing Analysis Models in Parallel <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Launched 3 concurrent <code>advanced_signals.py</code> processes:</p></div><div class="ev-body"><p>Launched 3 concurrent <code>advanced_signals.py</code> processes:</p>
<ul>
<li><strong>OCR/Text Detection</strong> (EasyOCR) — 793/9,011 done, continuing from previous partial run</li>
<li><strong>Image Captions</strong> (BLIP) — 0/9,011, loading model on MPS</li>
<li><strong>Facial Emotions</strong> (DeepFace) — FAILED: <code>No module named 'tensorflow'</code></li>
</ul>
<p>Installing TensorFlow + DeepFace to unblock the emotions phase. The other two processes are running in parallel, each writing to separate DB tables so no conflicts. OCR uses CPU (EasyOCR limitation on MPS), BLIP runs on Apple Silicon GPU.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>16:15 — Drift Page: Vector Nearest Neighbor Visualization <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Built a new <code>/drift</code> page for the dashboard. The concept: sample 10 random images, and for each one, show the 4 nearest neighbors according to each of the 3 embedding models (DINOv2, SigLIP, CLIP). This creates a visual comparison of what each model "sees" — DINOv2 finds composition and texture similarity, SigLIP finds semantic meaning, CLIP matches subjects.</p></div><div class="ev-body"><p>Built a new <code>/drift</code> page for the dashboard. The concept: sample 10 random images, and for each one, show the 4 nearest neighbors according to each of the 3 embedding models (DINOv2, SigLIP, CLIP). This creates a visual comparison of what each model "sees" — DINOv2 finds composition and texture similarity, SigLIP finds semantic meaning, CLIP matches subjects.</p>
<p>Implementation: <code>render_drift()</code> function queries LanceDB directly (0.02s per search), serves thumbnails via new <code>/thumb/{uuid}</code> endpoint from <code>rendered/thumb/jpeg/</code>. Each section is a card with 3 rows (one per model), showing the query image (blue border) and 4 neighbors with L2 distance overlays. Design uses existing Apple HIG design tokens. "Reshuffle" link reloads for a new random sample.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span></div><h3>14:30 — Dashboard Redesign: Apple HIG Design System + Dark Mode + HF-Style Tags <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> The old dashboard used monospace fonts, hardcoded hex colors, raw pixel values, and a flat monochrome aesthetic. The user wanted a proper design system with Apple rigor, a dark mode toggle, and HuggingFace-style tags with category icons.</p></div><div class="ev-body"><p><strong>Intent.</strong> The old dashboard used monospace fonts, hardcoded hex colors, raw pixel values, and a flat monochrome aesthetic. The user wanted a proper design system with Apple rigor, a dark mode toggle, and HuggingFace-style tags with category icons.</p>
<p><strong>What changed.</strong> Complete rewrite of the dashboard's visual layer in <code>generate_status_page.py</code>:</p>
<p><em>Design Token System (Apple HIG):</em></p>
<ul>
<li><strong>Typography</strong>: SF Pro Display for headings, SF Pro Text for body, SF Mono for data. 8-step type scale (11px to 34px) matching Apple HIG.</li>
<li><strong>Spacing</strong>: 4px-based scale (--space-1 through --space-16). Every margin, padding, and gap uses tokens.</li>
<li><strong>Colors</strong>: Apple system palette (blue, green, indigo, orange, pink, purple, red, teal, yellow, mint, cyan, brown) as CSS variables.</li>
<li><strong>Radius</strong>: 4-level scale (6px, 10px, 14px, 20px, 9999px). Cards get --radius-lg, badges get --radius-sm, pills get --radius-full.</li>
<li><strong>Shadows</strong>: 3 levels (sm, md, lg) that adapt to dark mode.</li>
<li><strong>Transitions</strong>: Shared easing curve and duration tokens.</li>
</ul>
<p><em>Dark/Light Theme:</em></p>
<ul>
<li>Theme toggle in sidebar bottom (sun/moon icon). State persists via localStorage.</li>
<li>Light theme default. All 25+ semantic color tokens switch between themes via <code>[data-theme]</code> selectors.</li>
<li>Theme-aware: badges, tags, cards, tables, progress bars, JSON syntax highlighting all adapt.</li>
</ul>
<p><em>HuggingFace-Style Tags:</em></p>
<ul>
<li>New <code>.tag</code> component with colored icon square + label + count. Replaces flat <code>.pill</code> class.</li>
<li>14 icon categories with Apple-colored tinted backgrounds: camera (blue), eye (indigo), palette (orange), sun (yellow), location (pink), scene (green), mood (purple), time (teal), style (pink), depth (mint), object (cyan), face (brown), format (gray), film (red).</li>
<li>Dominant color tags use actual color dots instead of icons.</li>
</ul>
<p><em>New Dashboard Section — Advanced Signals:</em></p>
<ul>
<li><strong>Depth Estimation</strong>: Animated near/mid/far percentage bar (blue/teal/indigo), complexity buckets as tags. Shows 9,011 images analyzed, avg near 54.2%, mid 25.1%, far 20.8%.</li>
<li><strong>Scene Classification</strong>: Top 15 scenes as tags, environment breakdown (indoor/outdoor/unknown). 9,011 classified.</li>
<li><strong>Enhancement Engine</strong>: Status tags showing all 9,011 images enhanced.</li>
<li><strong>Locations</strong>: Source breakdown, GPS from EXIF count (1,820).</li>
</ul>
<p><em>Token Audit:</em></p>
<ul>
<li>All sub-pages (Journal, Mosaics, Instructions, Blind Test) migrated from hardcoded hex/rem to design tokens.</li>
<li>Zero raw hex colors in PAGE_HTML CSS. Zero raw pixel values outside token definitions.</li>
<li>All <code>rgba()</code> values are intentional opacity modifiers for overlays/shadows, not standalone colors.</li>
</ul>
<p><em><code>get_stats()</code> extended:</em></p>
<ul>
<li>15 new data fields: aesthetic_count/avg/min/max/labels, depth_count/avg_near/mid/far/complexity_buckets, scene_count/top_scenes/scene_environments, enhancement_count/statuses, location_count/sources/accepted.</li>
<li>Total stats dict: 70 keys (was ~55).</li>
</ul>
<p><strong>Discovered.</strong> Scene classification already ran to completion (9,011 images) since last check. Aesthetic scores show poor discrimination — all 9,011 images rated "excellent" with scores 8.22-10.0. This model needs recalibration.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span></div><h3>18:30 — MADCurator Major Upgrade: Location Intelligence + All Signals + Power UX <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> Transform MADCurator from a curation-only tool into the ultimate image intelligence console. Every signal the pipeline has collected (EXIF, aesthetic scores, depth maps, scene classification, style labels, captions, OCR, emotions, enhancements, face/object counts) should be visible in the detail panel. Add a location system with GPS pre-population, manual tagging, and temporal propagation. Add enhanced image comparison. Add power keyboard shortcuts for speed.</p></div><div class="ev-body"><p><strong>Intent.</strong> Transform MADCurator from a curation-only tool into the ultimate image intelligence console. Every signal the pipeline has collected (EXIF, aesthetic scores, depth maps, scene classification, style labels, captions, OCR, emotions, enhancements, face/object counts) should be visible in the detail panel. Add a location system with GPS pre-population, manual tagging, and temporal propagation. Add enhanced image comparison. Add power keyboard shortcuts for speed.</p>
<p><strong>What changed across 9 files:</strong></p>
<p><em>Python (1 file):</em> <code>mad_database.py</code> — New <code>image_locations</code> table (uuid, location_name, lat/lon, source, confidence, propagated_from, accepted).</p>
<p><em>Swift (8 files):</em></p>
<ul>
<li><strong>Models.swift</strong> — PhotoItem grew from ~25 fields to ~55 fields. Added location (7 fields), aesthetic score (2), depth estimation (4), scene classification (6), style (2), caption, OCR, emotions, EXIF date/GPS, enhancement metrics (7), detection counts (2). New computed properties: <code>aestheticStars</code>, <code>aestheticBucket</code>, <code>scenesList</code>, <code>hasLocation</code>, <code>hasEnhancement</code>, <code>hasOCRText</code>. New filter dimensions: location, style, aesthetic, hasText.</li>
</ul>
<ul>
<li><strong>Database.swift</strong> — <code>loadPhotos()</code> query now JOINs 9 tables (was 3): images, gemini_analysis, tiers(x2), image_locations, aesthetic_scores, depth_estimation, scene_classification, style_classification, image_captions, exif_metadata, enhancement_plans. Plus 4 correlated subqueries for object/face counts, OCR text aggregation, and emotion summaries. New methods: <code>setLocation()</code>, <code>propagateLocation()</code> (temporal scoring: same-day=0.95, ±1d=0.85, ±3d=0.70, ±7d=0.60), <code>acceptLocation()</code>, <code>rejectLocation()</code>.</li>
</ul>
<ul>
<li><strong>PhotoStore.swift</strong> — 4 new filter dimensions with faceted counts. <code>showEnhanced</code> toggle state. <code>isFullscreen</code> mode. <code>showInfoPanel</code> toggle. <code>currentImagePath()</code> switches between display and enhanced tier. Location set/accept/reject with automatic propagation + data reload. Search now includes location, caption, OCR text.</li>
</ul>
<ul>
<li><strong>FilterSidebar.swift</strong> — 4 new sections: Location (mappin.and.ellipse icon), Style (theatermasks), Aesthetic (star, with Excellent/Good/Average/Poor buckets), Has Text (text.viewfinder, boolean yes/no).</li>
</ul>
<ul>
<li><strong>DetailView.swift</strong> — 9 new signal sections: EXIF (date taken + GPS coords), Location (editable field + confirm, propagated accept/reject), Caption (BLIP italic text), Aesthetic (5-star rating with orange stars), Style (purple badge + confidence), Scene (top 3 as pills with percentages), Depth Map (near/mid/far colored percentage bars), Enhancement (before→after metrics with delta arrows), OCR (quoted block with yellow accent), Emotions (pills), Detections (face/object counts). Enhanced/Original badge on hero image. Enhanced toggle button in curation bar.</li>
</ul>
<ul>
<li><strong>ContentView.swift</strong> — New keyboard shortcuts: E (toggle enhanced), Space (fullscreen), I (toggle info panel), Y (accept propagated location), N (reject propagated location). Fullscreen mode renders black background with just the image. Info panel toggle shows image-only view.</li>
</ul>
<ul>
<li><strong>ImageGrid.swift</strong> — Location pin icon (mappin.circle.fill) on thumbnails for geolocated images. Aesthetic score indicator (top-right corner) color-coded green/orange/red.</li>
</ul>
<ul>
<li><strong>MADCuratorApp.swift</strong> — New View menu: Toggle Enhanced (E), Toggle Info Panel (I), Toggle Fullscreen (Space), Focus Search (Cmd+F).</li>
</ul>
<p><strong>Build.</strong> All 8 Swift files compile cleanly in 4.11 seconds. Zero warnings, zero errors.</p>
<p>MADCurator now surfaces every signal collected by the pipeline. 55 data fields per image, 18 filter dimensions, 11 keyboard shortcuts.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span></div><h3>17:40 — Dashboard Responsive + GitHub Pages Deployment <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> The dashboard (<code>docs/index.html</code>) is what shows on GitHub Pages. It needed to be fully responsive for mobile/tablet, and show the timestamp of when it was last generated. Also needed a GitHub Actions workflow to auto-deploy on push.</p></div><div class="ev-body"><p><strong>Intent.</strong> The dashboard (<code>docs/index.html</code>) is what shows on GitHub Pages. It needed to be fully responsive for mobile/tablet, and show the timestamp of when it was last generated. Also needed a GitHub Actions workflow to auto-deploy on push.</p>
<p><strong>What changed.</strong> Added mobile breakpoints: tables get horizontal scroll wrapper (<code>.table-wrap</code>) so they don't break layout on narrow screens. Stats grid goes to single column below 440px. Stat card values shrink on mobile. Static build now embeds generation timestamp in the subtitle ("snapshot 2026-02-06 13:41 UTC"). Sidebar links redirect to GitHub URLs in static mode (no server routes). Created <code>.github/workflows/deploy-dashboard.yml</code> — deploys <code>docs/</code> to GitHub Pages on push to main.</p>
<p>Dashboard is responsive. Workflow ready. Run <code>python3 generate_status_page.py</code> before pushing to update the snapshot.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>17:30 — Full Enhancement Run <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("I want it all")</span><div class="ev-summary"><p><strong>Intent.</strong> Run the enhancement engine on every single photograph in the collection.</p></div><div class="ev-body"><p><strong>Intent.</strong> Run the enhancement engine on every single photograph in the collection.</p>
<p><strong>Results.</strong> 9,256 images enhanced in 282 seconds. Zero errors. 32.8 images/second with 8 workers. Every image now has a camera-aware enhanced copy at <code>rendered/enhanced/jpeg/{uuid}.jpg</code> (2048px, JPEG quality 92).</p>
<p>Before/after metrics confirm camera-specific corrections are working as designed:</p>
<ul>
<li><strong>Leica M8</strong> (3,533 images): WB shift +0.090 → +0.047 (47% correction)</li>
<li><strong>Leica Monochrom</strong> (1,099 images): WB unchanged at 0.000 (never touched)</li>
<li><strong>Canon G12</strong> (137 images): WB shift +0.167 → +0.060 (64% correction, most aggressive)</li>
<li><strong>Leica MP</strong> (1,126 images): WB shift +0.063 → +0.038 (40% correction, preserving film warmth)</li>
<li><strong>DJI Osmo Pro</strong> (3,032 images): WB shift +0.057 → +0.026 (54% correction)</li>
</ul>
<p>Steps applied across the collection: 78% WB correction, 100% sharpening, 49% saturation, 44% shadow/highlight recovery, 42% contrast, 37% exposure correction. Each recipe is saved in the <code>enhancement_plans</code> table with full before/after metrics.</p>
<p><code>rendered/enhanced/jpeg/</code> — 9,256 enhanced images. Ready for review in MADCurator.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>17:20 — Gemini Re-authenticated <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> Resume the stuck Gemini analysis (5,039/9,011 done, 3,902 remaining). User re-ran <code>gcloud auth application-default login</code>. Vertex AI client verified working. Relaunched <code>photography_engine.py</code> in background.</p></div><div class="ev-body"><p><strong>Intent.</strong> Resume the stuck Gemini analysis (5,039/9,011 done, 3,902 remaining). User re-ran <code>gcloud auth application-default login</code>. Vertex AI client verified working. Relaunched <code>photography_engine.py</code> in background.</p>
<p>PID 92782 running. 3,902 images to analyze.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>17:15 — The Endgame Vision <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("The incredible experience")</span><div class="ev-summary"><p><strong>Intent.</strong> A fundamental clarification of the project's architecture. Two audiences, two apps, one pipeline.</p></div><div class="ev-body"><p><strong>Intent.</strong> A fundamental clarification of the project's architecture. Two audiences, two apps, one pipeline.</p>
<p><strong>The private side</strong>: MADCurator (native SwiftUI app) is the review tool. The user examines every image — original, enhanced, AI variants — and accepts or rejects. This is where curation happens: the human eye decides what's worth showing.</p>
<p><strong>The public side</strong>: The web gallery (La Grille, La Dérive, Les Couleurs) shows ONLY accepted images. No pending, no rejected. The experience is curated. Every photograph that makes it to the public gallery was looked at, considered, and chosen.</p>
<p><strong>New images</strong>: The collection grows. New photographs get dropped into <code>originals/</code>. The pipeline handles incremental ingestion: register → render tiers → pixel analysis → Gemini analysis → signal extraction → vector embeddings → enhancement → curation. Every script already supports incremental mode (skip existing, process new).</p>
<p>This is the architecture going forward: signal everything, enhance everything, curate selectively, publish only the best.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>17:00 — The Enhancement Engine <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("Pure signal-driven corrections")</span><div class="ev-summary"><p><strong>Intent.</strong> Every image has different problems. A warm Portra night shot needs different treatment than an IR-contaminated M8 frame. The Canon G12 has the worst auto WB. The Monochrom sensor is pure B&W — never touch color. We use all the signals we collected (pixel analysis, camera body, medium, film stock) to compute per-image recipes, not batch presets. No AI, no style transfer — pure deterministic corrections.</p></div><div class="ev-body"><p><strong>Intent.</strong> Every image has different problems. A warm Portra night shot needs different treatment than an IR-contaminated M8 frame. The Canon G12 has the worst auto WB. The Monochrom sensor is pure B&W — never touch color. We use all the signals we collected (pixel analysis, camera body, medium, film stock) to compute per-image recipes, not batch presets. No AI, no style transfer — pure deterministic corrections.</p>
<p><strong>Architecture.</strong> New script <code>enhance_engine.py</code> with 6 camera profiles (<code>CameraProfile</code> dataclass) and 6 processing steps per image:</p>
<p>1. <strong>White Balance</strong> — Grey-world channel scaling. Strength varies: G12 at 0.7 (aggressive), M8 at 0.5 (careful — some warmth is CCD character), MP/Portra at 0.3 (preserve film warmth). Monochrom: skip entirely.</p>
<p>2. <strong>Exposure</strong> — Gamma correction toward 110-120 brightness. Guards against correcting intentional low-key/high-key. Film gets gentler correction.</p>
<p>3. <strong>Shadow/Highlight Recovery</strong> — Selective tone curve. Lifts crushed shadows, pulls blown highlights. Monochrom exception: only recover if clipping > 30% (heavy shadows are stylistic).</p>
<p>4. <strong>Contrast</strong> — Adaptive S-curve applied to luminance only (preserves color). Strength from 0 (skip) to 0.6 (strong) based on measured contrast ratio.</p>
<p>5. <strong>Saturation</strong> — HSV scaling. Monochrom: skip. Portra: cap at 1.10x (already vivid). G12: up to 1.20x (compact cameras are flat).</p>
<p>6. <strong>Noise-Aware Sharpening</strong> — Pillow UnsharpMask. Film (noise>3): radius=0.8, percent=40 (preserve grain). Clean digital: radius=1.5, percent=80. Monochrom: crisp edges.</p>
<p><strong>Results.</strong> 20-image test batch at 17 images/second, 0 errors. Camera-specific corrections verified:</p>
<ul>
<li>M8: WB shift reduced from +0.085 to +0.040 (50% correction)</li>
<li>Monochrom: zero color change (WB untouched)</li>
<li>G12: WB reduced from +0.188 to +0.066 (aggressive 70%)</li>
<li>MP/Portra: WB from +0.412 to +0.275 (gentle 30% — preserving film warmth)</li>
</ul>
<p>New DB table <code>enhancement_plans</code> stores every recipe as queryable JSON with pre/post metrics. Output: <code>rendered/enhanced/jpeg/{uuid}.jpg</code> at 2048px for review.</p>
<p>Created <code>enhance_engine.py</code>, added <code>enhancement_plans</code> table to <code>mad_database.py</code>. Ready for full 9,011-image run.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>16:45 — Gemini Processing Blocked <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> Resume Gemini analysis (55.9% complete, 3,972 images pending). Attempted restart but GCP Application Default Credentials have expired.</p></div><div class="ev-body"><p><strong>Intent.</strong> Resume Gemini analysis (55.9% complete, 3,972 images pending). Attempted restart but GCP Application Default Credentials have expired.</p>
<p><strong>Status.</strong> <code>photography_engine.py</code> fails immediately with "Reauthentication is needed. Please run <code>gcloud auth application-default login</code>". All local/programmatic analysis is complete (EXIF, pixel, colors, faces, objects, hashes, vectors). Only Gemini semantic analysis remains blocked on re-authentication.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>16:30 — System Instructions Page <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> As the project grows, development principles need to be documented where both the user and the AI assistant can reference them. Not in a CLAUDE.md that only the AI sees — in the dashboard, visible to everyone.</p></div><div class="ev-body"><p><strong>Intent.</strong> As the project grows, development principles need to be documented where both the user and the AI assistant can reference them. Not in a CLAUDE.md that only the AI sees — in the dashboard, visible to everyone.</p>
<p><strong>What changed.</strong> Created <code>render_instructions()</code> function with comprehensive development guidelines organized into 8 sections: Vision (signal augmentation philosophy), Signal Completeness (every image gets every signal), Performance (batch processing, MPS acceleration, incremental work), Data Integrity (no duplicates, no orphans, flat layout), Code Quality (Python 3.9, type hints, error handling), AI Analysis (Gemini guidelines, camera-aware processing), Dashboard & Monitoring (real-time stats, journal discipline), and Current Signal Inventory (table of all 12+ signals with source/status). Added <code>/instructions</code> route and sidebar link on all pages.</p>
<p>The project now has a living reference document accessible at http://localhost:8080/instructions.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-pink)">UI/UX</span></div><h3>16:15 — Mosaic Generation <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> See all 9,011 photographs at once — not scrolling through a grid, but tiled into one 4096px square image. Like a satellite view of the collection. Different sort orders reveal different patterns: sort by brightness and you see a gradient from black to white; sort by hue and you see a rainbow; sort by category and you see camera-specific color signatures.</p></div><div class="ev-body"><p><strong>Intent.</strong> See all 9,011 photographs at once — not scrolling through a grid, but tiled into one 4096px square image. Like a satellite view of the collection. Different sort orders reveal different patterns: sort by brightness and you see a gradient from black to white; sort by hue and you see a rainbow; sort by category and you see camera-specific color signatures.</p>
<p><strong>What changed.</strong> Created <code>generate_mosaics.py</code> — reads micro tier (64px) thumbnails, arranges them in a square grid (~95×95 at 43px tiles), saves 4096px JPEG mosaics. 14 sort variants: random, by_category, by_camera, by_brightness, by_hue, by_saturation, by_colortemp, by_dominant_color, by_contrast, by_sharpness, by_time_of_day, by_grading, by_faces, by_latitude. Dimensions with partial data (time_of_day: 5,039 images, latitude: 1,820) produce smaller mosaics. Metadata saved to <code>mosaics.json</code>. Added <code>/mosaics</code> route and gallery page to dashboard.</p>
<p>14 mosaics totaling 93 MB in <code>rendered/mosaics/</code>. The by_hue mosaic is a particularly beautiful rainbow. The by_latitude mosaic (1,820 GPS-tagged images) reveals geographic patterns in shooting style.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-orange)">Investigation</span></div><h3>16:00 — Multi-Page Architecture <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> The dashboard sidebar navigation was only on the main page. The README, Journal, Mosaics, and Blind Test pages were standalone HTML — no sidebar, no consistent navigation. The user wanted the same left menu on every page.</p></div><div class="ev-body"><p><strong>Intent.</strong> The dashboard sidebar navigation was only on the main page. The README, Journal, Mosaics, and Blind Test pages were standalone HTML — no sidebar, no consistent navigation. The user wanted the same left menu on every page.</p>
<p><strong>What changed.</strong> Created <code>page_shell(title, content, active="")</code> — a shared HTML wrapper that provides the sidebar + flex layout for any sub-page. The sidebar highlights the active page. Updated <code>render_readme()</code>, <code>render_journal()</code>, and <code>render_mosaics()</code> to use <code>page_shell()</code> instead of standalone templates. Journal page preserves its markdown-specific CSS via embedded <code><style></code> block.</p>
<p>All pages now share one navigation UI. The dashboard feels like one app, not separate pages.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>15:30 — Unified Pill/Tag Design System <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> Every data dimension — grading, time of day, setting, categories, cameras, vibes, colors, objects — used a different visual format: some tables, some inline text, some badges. They all represent the same thing: a filterable label with a count. The user pointed out these will become clickable filters, so they need one consistent format.</p></div><div class="ev-body"><p><strong>Intent.</strong> Every data dimension — grading, time of day, setting, categories, cameras, vibes, colors, objects — used a different visual format: some tables, some inline text, some badges. They all represent the same thing: a filterable label with a count. The user pointed out these will become clickable filters, so they need one consistent format.</p>
<p><strong>What changed.</strong> Converted all data sections from <code>rows()</code> (table format) to <code>pills()</code> (dark background, white text, rounded corners). Pill CSS: <code>background: var(--fg)</code>, label bold, count semi-transparent. Color pills special-cased with actual colored circles from averaged RGB values. Section title hierarchy: parent headings (GEMINI INSIGHTS, CAMERA FLEET) are black, larger, bold with bottom border; sub-headings (Grading, Time of Day) are smaller, muted gray. Layout: Gemini Insights first two rows in three-column grid.</p>
<p>Every data point in the dashboard now speaks the same visual language. Ready for filter interaction.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>15:10 — Dashboard Polish <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>Render Tiers table now shows each tier/format separately (e.g. <code>display/jpeg</code>, <code>display/webp</code>) instead of trying to merge them. Removed the Recent Analyses section (Sample Gemini Output is sufficient). Color pills render as actual colored circles with counts. Object detection shows real YOLO labels.</p></div><div class="ev-body"><p>Render Tiers table now shows each tier/format separately (e.g. <code>display/jpeg</code>, <code>display/webp</code>) instead of trying to merge them. Removed the Recent Analyses section (Sample Gemini Output is sufficient). Color pills render as actual colored circles with counts. Object detection shows real YOLO labels.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span><span class="ev-label" style="--label-color:var(--apple-pink)">UI/UX</span></div><h3>15:00 — Signal Extraction Complete <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>All 5 phases finished in 21 minutes (1,256s). Final results:</p></div><div class="ev-body"><p>All 5 phases finished in 21 minutes (1,256s). Final results:</p>
<ul>
<li><strong>EXIF metadata</strong>: 9,011 rows (1,820 with GPS)</li>
<li><strong>Dominant colors</strong>: 45,051 clusters (5 per image, K-means in LAB space)</li>
<li><strong>Face detection</strong>: 5,686 faces across 1,676 images (YuNet, 31 img/s)</li>
<li><strong>Object detection</strong>: 14,931 detections across 5,363 images (YOLOv8n, 29 img/s)</li>
<li><strong>Perceptual hashes</strong>: 9,276 rows with pHash/aHash/dHash/wHash + blur/sharpness/entropy</li>
</ul>
<p>Top objects: person (4,752), car (3,603), traffic light (1,051), cat (977). Dashboard now shows actual color pills (colored circles from average RGB per color name) instead of text, and object labels correctly.</p>
<p>Every photograph now has: EXIF, 5 dominant colors, face boxes, object labels, 4 perceptual hashes, quality metrics. Combined with Gemini analysis + pixel analysis + 3 vector embeddings = comprehensive signal coverage.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>14:30 — Signal Extraction Progress Check <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p>The 5-phase signal extraction (launched previous session) is running through 9,011 images:</p></div><div class="ev-body"><p>The 5-phase signal extraction (launched previous session) is running through 9,011 images:</p>
<ul>
<li><strong>EXIF metadata</strong>: 9,011/9,011 — complete (1,820 with GPS coordinates)</li>
<li><strong>Dominant colors</strong>: 45,051 rows (9,011 × 5 clusters) — complete</li>
<li><strong>Face detection</strong>: 3,187 faces found so far — in progress</li>
<li><strong>Object detection</strong>: 1,418 objects found so far — in progress</li>
<li><strong>Perceptual hashes</strong>: pending</li>
</ul>
<p>Still running. YuNet face detection and YOLOv8 object detection processing through the collection.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>14:20 — Dashboard Left Sidebar Navigation & Tier Format Fix <span class="ev-expand-hint">&#9656;</span></h3><div class="ev-summary"><p><strong>Intent.</strong> Two user requests: (1) the Render Tiers table showed file counts nearly double the image counts with no explanation — needed to clarify that display/mobile/thumb/micro tiers produce both JPEG and WebP; (2) add a persistent left sidebar navigation to access all dashboard sections and the Journal without scrolling.</p></div><div class="ev-body"><p><strong>Intent.</strong> Two user requests: (1) the Render Tiers table showed file counts nearly double the image counts with no explanation — needed to clarify that display/mobile/thumb/micro tiers produce both JPEG and WebP; (2) add a persistent left sidebar navigation to access all dashboard sections and the Journal without scrolling.</p>
<p><strong>What changed.</strong> Layout restructured from single centered column to <code>display: flex</code> with a 200px sticky sidebar + main content area. Sidebar has grouped links (Analysis, Insights, Pipeline, Data) with scroll-spy highlighting that tracks the active section. All 13 sections got anchor IDs. The Render Tiers table gained JPEG/WebP columns and an explanatory note. Responsive: on mobile the sidebar collapses to a horizontal link bar. Also verified camera-friendly subcategory names are working (Leica Digital, Leica Analog, Leica Monochrom, Canon G12, DJI Osmo Pro, DJI Osmo Memo).</p>
<p>Dashboard now has proper navigation. Tier breakdown shows: full/gemini/original = JPEG only, display/mobile/thumb/micro = JPEG + WebP.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>13:30 — Full System Dashboard <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("Show me all that is there, all the stats")</span><div class="ev-summary"><p><strong>Intent.</strong> The original dashboard showed Gemini analysis progress, category tables, render tiers, and variant generation — about 40% of the system. Missing: camera fleet with per-body pixel metrics, pixel analysis distributions (color cast, color temperature), vector store status, curation progress, Gemini semantic insights (vibes, time of day, setting, exposure, composition), source format breakdown, and storage usage. The user wanted one page that shows everything.</p></div><div class="ev-body"><p><strong>Intent.</strong> The original dashboard showed Gemini analysis progress, category tables, render tiers, and variant generation — about 40% of the system. Missing: camera fleet with per-body pixel metrics, pixel analysis distributions (color cast, color temperature), vector store status, curation progress, Gemini semantic insights (vibes, time of day, setting, exposure, composition), source format breakdown, and storage usage. The user wanted one page that shows everything.</p>
<p><strong>What changed.</strong> Complete rewrite of <code>generate_status_page.py</code>. The <code>get_stats()</code> function now collects 40+ fields from 6 tables plus LanceDB. The HTML template gained 8 new sections: top stat cards row (8 cards with sub-labels and status badges), Camera Fleet table (body, count, medium, film stock, luminance, WB shifts color-coded red/blue, noise, shadow clip), Pixel Analysis (color cast pills with colored dots, color temperature distribution), Vector Store (3 model cards with descriptions, row count, disk size, completion badge), Gemini Insights (3-column grading/time/setting tables, exposure/composition/vibe/rotation pills), Curation progress, Storage summary, and source format breakdown.</p>
<p>Rewrote <code>generate_status_page.py</code> — 1,635 lines (was 1,367). 8 stat cards, 13 sections, live-polling every 5s. All routes preserved: <code>/journal</code>, <code>/blind-test</code>.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>13:00 — Full Vector Extraction <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("go")</span><div class="ev-summary"><p><strong>Intent.</strong> Run all 9,276 images through all three models. The 20-image test proved the pipeline works — time to fill the database.</p></div><div class="ev-body"><p><strong>Intent.</strong> Run all 9,276 images through all three models. The 20-image test proved the pipeline works — time to fill the database.</p>
<p><strong>Results.</strong> 9,011 images vectorized (265 skipped — no display tier file). All three models completed on MPS:</p>
<div class="table-wrap"><table><thead><tr class="thead"><th>Model</th><th>Vectors</th><th>Time</th><th>Dimension</th></tr></thead><tbody>
<tr class=""><td>DINOv2</td><td>9,011</td><td>6m 12s</td><td>768</td></tr>
<tr class=""><td>SigLIP</td><td>9,011</td><td>5m 47s</td><td>768</td></tr>
<tr class=""><td>CLIP</td><td>9,011</td><td>5m 28s</td><td>512</td></tr>
<tr class=""><td><strong>Total</strong></td><td><strong>9,011 triples</strong></td><td><strong>17.6 min</strong></td><td>—</td></tr>
</tbody></table></div>
<p>Processing rate: 8.8 images/second across all three models. LanceDB stores 9,011 rows with proper <code>FixedSizeList<float32></code> columns. Similarity search, text search, and duplicate detection all verified working on the full dataset.</p>
<p><code>vectors.lance/</code> — 9,011 complete vector triples. Ready for La Dérive integration.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>12:30 — Vector Engine <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("Store 3 vectors for each image for later use in navigation")</span><div class="ev-summary"><p><strong>Intent.</strong> The web gallery's La Dérive experience drifts through connected photographs — but the connections were computed from shared vibes and colors, which is shallow. Real visual similarity requires embeddings from models that actually <em>see</em> the image. Three different models for three different kinds of seeing:</p></div><div class="ev-body"><p><strong>Intent.</strong> The web gallery's La Dérive experience drifts through connected photographs — but the connections were computed from shared vibes and colors, which is shallow. Real visual similarity requires embeddings from models that actually <em>see</em> the image. Three different models for three different kinds of seeing:</p>
<ul>
<li><strong>DINOv2</strong> (<code>facebook/dinov2-base</code>, 768d) — self-supervised vision transformer trained without labels. Sees composition, texture, spatial layout. Two images with similar geometric arrangements score high even if the subjects differ. This is the "artistic eye."</li>
<li><strong>SigLIP</strong> (<code>google/siglip-base-patch16-224</code>, 768d) — multimodal model with shared image/text embedding space. Sees meaning: "golden hour portrait" or "rainy street" as concepts. Enables text-to-image search. This is the "semantic brain."</li>
<li><strong>CLIP</strong> (<code>openai/clip-vit-base-patch32</code>, 512d) — similar to SigLIP but optimized for precise subject matching. Two photos of the same building score very high. This is the "duplicate detector."</li>
</ul>
<p><strong>Architecture.</strong> <code>vector_engine.py</code> processes one model at a time (to fit in memory), extracts L2-normalized vectors on Apple Silicon MPS, stores them in LanceDB as FixedSizeList float32 arrays. PyArrow schema ensures proper vector types for cosine similarity search. Incremental processing — only new images get vectorized.</p>
<p><strong>Modes:</strong> <code>--search UUID</code> (find similar via all 3 models), <code>--text "query"</code> (semantic search via SigLIP), <code>--duplicates 0.95</code> (find near-dupes via CLIP).</p>
<p><strong>Dependencies installed:</strong> PyTorch 2.8.0 (MPS), Transformers 4.57.6, LanceDB 0.27.1, sentencepiece, protobuf. All models verified working on MPS with 20 test images. Ready for full 9,276-image extraction.</p>
<p>Created <code>vector_engine.py</code>. Tested extraction + LanceDB storage + similarity search on 20 images. Three distinct similarity rankings confirmed — each model sees differently.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>12:00 — Apple-Grade Design Upgrade <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("Elevate MADCurator to Apple HIG standards")</span><div class="ev-summary"><p><strong>Intent.</strong> The functional app worked but looked utilitarian. The photographs deserve a frame that does them justice — polished interactions, refined materials, meaningful animations. Photography-first design where the UI recedes and the images breathe.</p></div><div class="ev-body"><p><strong>Intent.</strong> The functional app worked but looked utilitarian. The photographs deserve a frame that does them justice — polished interactions, refined materials, meaningful animations. Photography-first design where the UI recedes and the images breathe.</p>
<p><strong>What changed across 6 files:</strong></p>
<p><em>Models.swift</em> — Added <code>SemanticPop</code> struct with color-to-NSColor mapping, <code>paletteColors</code> computed property (parses hex from Gemini's raw_json color palette), <code>semanticPopsList</code> parser, and <code>NSColor.fromHex</code> extension. Also added <code>colorPaletteJSON</code> field loaded from DB via <code>json_extract()</code>.</p>
<p><em>ImageGrid.swift</em> — Grid now breathes: minimum 160px/maximum 240px cells with 4pt spacing. Thumbnails use <code>.fit</code> instead of <code>.fill+clip</code> to show actual composition. Hover effect with subtle 1.02 scale + shadow via spring animation. Selection replaced hard border with rounded overlay ring + spring. Rejected photos fade to 0.3 AND desaturate. Right-click context menu: Keep/Reject/Copy UUID.</p>
<p><em>DetailView.swift</em> — Hero image fills width with no height cap, black surround. Camera badge shows SF Symbol + body name + film stock inline. Color palette as 5 colored circles (the requested color pills). Semantic pops as colored dot + object label in pills. Alt text in quoted block style with accent-colored left border. Vibes rendered as glass pills using <code>.ultraThinMaterial</code> with subtle border. Curation buttons wider with press scale animation. Section headers now have SF Symbol icons. Spacing increased to 20pt between sections.</p>
<p><em>FilterSidebar.swift</em> — Every section gets an SF Symbol icon (camera, paintpalette, sparkles, clock, mappin, cube, etc). Active sections show accent-colored icon + dot indicator. Filter chips darken on hover. Search field taller with clear button. Sidebar uses <code>.regularMaterial</code> for vibrancy. <code>@FocusState</code> added for search field.</p>
<p><em>ContentView.swift</em> — Empty state shows camera.viewfinder icon + lighter weight text. Query bar operators styled as tiny pills. Active chips get subtle shadow. Toolbar moved to <code>.status</code> placement. Escape key deselects current photo. Removed duplicate onKeyPress handlers (menu commands handle k/r/arrows).</p>
<p><em>MADCuratorApp.swift</em> — Unified toolbar style. View menu with sidebar toggle (Cmd+Opt+S).</p>
<p>Built cleanly on first try. 6 files modified, 0 new files. The monospace aesthetic preserved throughout — it's intentional, not default.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-pink)">UI/UX</span><span class="ev-label" style="--label-color:var(--apple-indigo)">Architecture</span></div><h3>11:30 — Camera Filter in MADCurator <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("in the app I should see the filters for Camera")</span><div class="ev-summary"><p><strong>Intent.</strong> Now that every image knows its camera, the curator should let you filter by it. See all Leica MP shots together, compare Canon G12 against M8 side by side.</p></div><div class="ev-body"><p><strong>Intent.</strong> Now that every image knows its camera, the curator should let you filter by it. See all Leica MP shots together, compare Canon G12 against M8 side by side.</p>
<p>Added camera_body to PhotoItem, FilterDimension, FilterState, FacetedOptions. Added "Camera" section to FilterSidebar. Added Camera metadata section to DetailView (body, film stock, medium, monochrome). App rebuilt successfully.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>11:15 — Pixel-Level Analysis <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("run programatic image analysis")</span><div class="ev-summary"><p><strong>Intent.</strong> Gemini tells us <em>what's in the photo</em> — vibes, composition, mood. But for auto-enhance we need to know <em>what's wrong with the pixels</em>. Histogram shape, white balance deviation, contrast ratio, noise level, saturation distribution. Two complementary data sources: semantic (Gemini) + technical (pixel math).</p></div><div class="ev-body"><p><strong>Intent.</strong> Gemini tells us <em>what's in the photo</em> — vibes, composition, mood. But for auto-enhance we need to know <em>what's wrong with the pixels</em>. Histogram shape, white balance deviation, contrast ratio, noise level, saturation distribution. Two complementary data sources: semantic (Gemini) + technical (pixel math).</p>
<p><strong>Architecture.</strong> New script <code>image_analysis.py</code> reads each display-tier JPEG (2048px), converts to numpy arrays, and computes 20 metrics: luminance histogram (clipping, dynamic range, low/high key), channel means and WB shifts, color cast classification, HSV saturation, dominant hue via circular mean, Michelson contrast, Laplacian noise estimate. Results stored in new <code>image_analysis</code> table. 16-bin per-channel histograms stored as JSON for visualization.</p>
<p><strong>Results.</strong> 8,763 images analyzed at 28/s. The data immediately reveals camera-specific patterns:</p>
<div class="table-wrap"><table><thead><tr class="thead"><th>Camera</th><th>WB Red</th><th>Color Cast %</th><th>Noise</th><th>Shadow Clip</th></tr></thead><tbody>
<tr class=""><td>Leica M8</td><td>+0.091</td><td>66%</td><td>1.5</td><td>11.6%</td></tr>
<tr class=""><td>DJI Osmo Pro</td><td>+0.042</td><td>61%</td><td>1.4</td><td>1.8%</td></tr>
<tr class=""><td>Leica MP (Portra)</td><td>+0.063</td><td>68%</td><td>4.3</td><td>11.2%</td></tr>
<tr class=""><td>Leica Monochrom</td><td>0.000</td><td>0%</td><td>1.7</td><td>21.3%</td></tr>
<tr class=""><td>Canon G12</td><td>+0.167</td><td>80%</td><td>1.9</td><td>12.7%</td></tr>
</tbody></table></div>
<p>The Portra film grain (noise=4.3) is 3× higher than digital cameras — that's real silver halide texture we want to preserve, not denoise. The Canon G12 has the worst white balance (+0.167 red shift) and 80% of its images need correction. The Leica Monochrom confirms zero color cast, zero saturation — only tone curves needed.</p>
<p>Created <code>image_analysis.py</code>, added <code>image_analysis</code> table to schema. 8,763 images analyzed in ~310s.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>11:00 — Camera Provenance <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("analog where images taken with Leica MP camera with film")</span><div class="ev-summary"><p><strong>Intent.</strong> Every photograph has a camera behind it, and every camera has a personality. The Leica MP shoots Kodak Portra 400 VC — vivid color film that shifts warm under tungsten light, which explains the white balance problems on night shots. The Leica M8 has a CCD sensor with known IR contamination that adds magenta to dark fabrics. The Leica Monochrom has no Bayer filter — pure B&W sensor, never apply color corrections. The Canon G12 is a compact with the worst auto white balance in the set. The DJI Osmo Pro and Memo are action cameras with wide lenses.</p></div><div class="ev-body"><p><strong>Intent.</strong> Every photograph has a camera behind it, and every camera has a personality. The Leica MP shoots Kodak Portra 400 VC — vivid color film that shifts warm under tungsten light, which explains the white balance problems on night shots. The Leica M8 has a CCD sensor with known IR contamination that adds magenta to dark fabrics. The Leica Monochrom has no Bayer filter — pure B&W sensor, never apply color corrections. The Canon G12 is a compact with the worst auto white balance in the set. The DJI Osmo Pro and Memo are action cameras with wide lenses.</p>
<p><strong>Why it matters for auto-enhance.</strong> Generic color correction treats every image the same. But a warm-shifted Portra night shot needs different treatment than an IR-contaminated M8 frame. The camera body tells us <em>what kind of wrong</em> the image is. The film stock tells us <em>what kind of grain</em> is an asset vs. artifact. This is the difference between fixing and destroying.</p>
<p>Added <code>camera_body</code>, <code>film_stock</code>, <code>medium</code>, <code>is_monochrome</code> columns to <code>images</code> table. Built migration system in <code>mad_database.py</code>. Populated all 8,807 images from category/subcategory mapping. 77 Analog shots detected as monochrome via Gemini grading_style.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>02:10 — Fixing MADCurator <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("the vibe label look wrong with some ]")</span><div class="ev-summary"><p><strong>Intent.</strong> The native curator app was showing garbled vibe labels like <code>"Candid"]·11</code> instead of <code>Candid·11</code>.</p></div><div class="ev-body"><p><strong>Intent.</strong> The native curator app was showing garbled vibe labels like <code>"Candid"]·11</code> instead of <code>Candid·11</code>.</p>
<p><strong>Root cause.</strong> The <code>vibeList</code> computed property in <code>Models.swift</code> was splitting the vibe string on commas — but the DB stores a JSON array (<code>["Moody", "Nostalgic", "Stylish"]</code>). Splitting <code>["Moody", "Nostalgic", "Stylish"]</code> on <code>,</code> gives <code>["Moody"</code>, <code>"Nostalgic"</code>, <code>"Stylish"]</code>.</p>
<p><strong>Fix.</strong> Replaced comma-split with <code>JSONSerialization</code> parsing. Also added collapsible vibe filter: vibes with 5+ photos shown by default, rest behind "all X more" toggle. Updated <code>Database.swift</code> to load <code>tiers.local_path</code> from DB so thumbnails work regardless of file layout.</p>
<p>Fixed 3 files: Models.swift, FilterSidebar.swift, Database.swift + PhotoStore.swift.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>02:00 — The `rendered/originals/` Saga <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("why does this folder still exists?")</span><div class="ev-summary"><p><strong>Intent.</strong> Keep the rendered directory clean and organized. One layout, no duplicates, no confusion.</p></div><div class="ev-body"><p><strong>Intent.</strong> Keep the rendered directory clean and organized. One layout, no duplicates, no confusion.</p>
<p><strong>What went wrong.</strong> The render pipeline's <code>output_dir</code> defaulted to <code>rendered/originals/</code> — which kept recreating the folder after every deletion. Meanwhile, the first batch of images (5,138 JPEGs) was in a flat layout (<code>rendered/{tier}/jpeg/{uuid}.jpg</code>) and the DNG re-renders landed in a nested layout (<code>rendered/originals/{tier}/jpeg/{cat}/{sub}/{uuid}.jpg</code>). Two different layouts, two different folders, total mess.</p>
<p><strong>Lesson.</strong> Before re-running a pipeline that creates files, check where it outputs. Don't just purge DB entries and re-run — verify the <code>output_dir</code> matches the expected layout first.</p>
<p><strong>Resolution.</strong> Fixed <code>render_pipeline.py</code> to output directly to <code>rendered/</code> (not <code>rendered/originals/</code>). Removed category subdirectories from tier paths (flat layout: <code>rendered/{tier}/{fmt}/{uuid}.ext</code>). Moved 38,410 DNG tier files from nested to flat. Deleted <code>rendered/originals/</code> for good. The canonical layout is now:</p>
<pre><code>
rendered/
  {tier}/jpeg/{uuid}.jpg
  {tier}/webp/{uuid}.webp
  original/jpeg/{uuid}.jpg   ← native-resolution JPEG (only for JPEG-sourced)
</code></pre></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>01:45 — The DNG Purple Cast <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("this look like it is from DNG wrongly transformed")</span><div class="ev-summary"><p><strong>Intent.</strong> Render all 3,841 DNG files properly. They'd been through the pipeline but every image had a purple/magenta color cast.</p></div><div class="ev-body"><p><strong>Intent.</strong> Render all 3,841 DNG files properly. They'd been through the pipeline but every image had a purple/magenta color cast.</p>
<p><strong>Root cause.</strong> macOS <code>sips</code> converts DNG to TIFF in Display P3 color space. Pillow reads the pixels but saves to JPEG without converting to sRGB. Browsers and image viewers interpret the JPEG as sRGB, shifting reds and blues — hence the purple tint.</p>
<p><strong>Fix.</strong> Added <code>-m /System/Library/ColorSync/Profiles/sRGB Profile.icc</code> to the <code>sips</code> command in <code>_decode_raw_sips()</code>. This converts to sRGB at decode time. Verified: re-rendered DNGs look correct.</p>
<p>Fixed <code>render_pipeline.py</code>. Also switched <code>photography_engine.py</code> from API key to Vertex AI ADC (the key was the one that got committed and removed). Fixed <code>IMAGE_DIR</code> path and <code>find_gemini_jpeg</code> to match flat layout.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-orange)">Investigation</span></div><h3>01:30 — Three Experiences <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("Build me a web gallery with three ways to see the photos")</span><div class="ev-summary"><p><strong>Intent.</strong> The native curator app is for work — deciding what's good. But the photographs themselves deserve to be seen, explored, discovered. Not a grid-of-thumbnails photo gallery — three different ways to navigate through semantic space. La Grille (filter by vibes, grading, time, composition), La Dérive (drift through connected photos by shared meaning), Les Couleurs (explore by color palette and semantic pops).</p></div><div class="ev-body"><p><strong>Intent.</strong> The native curator app is for work — deciding what's good. But the photographs themselves deserve to be seen, explored, discovered. Not a grid-of-thumbnails photo gallery — three different ways to navigate through semantic space. La Grille (filter by vibes, grading, time, composition), La Dérive (drift through connected photos by shared meaning), Les Couleurs (explore by color palette and semantic pops).</p>
<p><strong>Architecture.</strong> New data export script (<code>export_gallery_data.py</code>) queries the 634 analyzed photos from SQLite, extracts palettes, vibes, semantic pops, and precomputes a drift connection graph — top 6 neighbors per photo scored by shared vibes, color proximity, matching objects, and same setting. Outputs a single <code>photos.json</code> (1.3 MB) with everything the frontend needs.</p>
<p><strong>Design.</strong> Dark (#0a0a0a), monospace, glassmorphism. No framework — vanilla HTML/CSS/JS. Glass tags with <code>backdrop-filter: blur(12px)</code> bloom on hover. Progressive image loading (micro → thumb → display). Justified row layout. Lazy loading via IntersectionObserver.</p>
<p>Built <code>export_gallery_data.py</code>, <code>serve_gallery.py</code> (port 3000), and 6 web files: <code>index.html</code>, <code>style.css</code>, <code>app.js</code>, <code>grid.js</code>, <code>drift.js</code>, <code>colors.js</code>. 634 photos with full semantic data. Three experiences ready for iteration.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span><span class="ev-label" style="--label-color:var(--apple-pink)">UI/UX</span></div><h3>00:30 — Faceted Search <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("Create a way better navigation system with union and intersection simple queries")</span><div class="ev-summary"><p><strong>Intent.</strong> The first sidebar was a vertical list of single-select pills — click one, see results, click another, lose the first. No multi-select, no compound queries, no visibility into what you're filtering. Scrolling through 15 sections of tags with no context was painful.</p></div><div class="ev-body"><p><strong>Intent.</strong> The first sidebar was a vertical list of single-select pills — click one, see results, click another, lose the first. No multi-select, no compound queries, no visibility into what you're filtering. Scrolling through 15 sections of tags with no context was painful.</p>
<p><strong>Solution.</strong> Proper faceted search: multi-select within each dimension (union/OR), intersection across dimensions (AND). Contextual counts that update in real-time — options with zero matches disappear. A query bar above the grid showing the active expression with <code>∪</code> and <code>∩</code> operators. Removable chips. For vibes: a toggle between "Any of these" and "All of these".</p>
<p>Rewrote 4 files (Models, PhotoStore, FilterSidebar, ContentView). FlowLayout chips with counts. Empty sections auto-hide. ~2ms faceted recomputation for 9k images.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>00:15 — MADCurator: A Native App <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("Create a native app so it is faster? Apple style/rigor")</span><div class="ev-summary"><p><strong>Intent.</strong> The curation interface needed to handle 9,011+ images with instant filtering, smooth scrolling, and keyboard-driven workflow. A web app would struggle. A native SwiftUI macOS app reads directly from the SQLite database, loads thumbnails from the rendered tier on disk, and keeps everything in-process.</p></div><div class="ev-body"><p><strong>Intent.</strong> The curation interface needed to handle 9,011+ images with instant filtering, smooth scrolling, and keyboard-driven workflow. A web app would struggle. A native SwiftUI macOS app reads directly from the SQLite database, loads thumbnails from the rendered tier on disk, and keeps everything in-process.</p>
<p>Built <code>MADCurator.app</code> — SwiftUI, NavigationSplitView with sidebar/grid/detail, SQLite3 C API, NSCache for 2000 thumbnails, Keep/Reject with K/R keys, arrow navigation.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-green)">Deploy</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>00:00 — Scrubbing the Secret <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("IMPORTANT push asap to remove my api keys")</span><div class="ev-summary"><p><strong>Intent.</strong> A Google API key had been committed in the initial git push as a fallback in <code>photography_engine.py</code>. It needed to go — not just from the current code, but from the entire git history. Every commit, every diff, every reflog entry.</p></div><div class="ev-body"><p><strong>Intent.</strong> A Google API key had been committed in the initial git push as a fallback in <code>photography_engine.py</code>. It needed to go — not just from the current code, but from the entire git history. Every commit, every diff, every reflog entry.</p>
<p>Installed <code>git-filter-repo</code>, rewrote all history to replace the key with <code>REDACTED_API_KEY</code>, removed the fallback entirely (now env-var-only), force-pushed the cleaned history to GitHub. Key revocation recommended.</p></div></div>
<h2 class="date-header">2026-02-05</h2>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>23:10 — Designing the Curation Interface <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("Create an interface to navigate the images with all the tags")</span><div class="ev-summary"><p><strong>Intent.</strong> With Gemini analyzing every photograph's exposure, composition, color palette, vibe, and setting — we have the metadata to make smart decisions. The interface should let a human quickly scan thousands of images, filter by any dimension, and reject the ones with no potential. Only the survivors get enhanced.</p></div><div class="ev-body"><p><strong>Intent.</strong> With Gemini analyzing every photograph's exposure, composition, color palette, vibe, and setting — we have the metadata to make smart decisions. The interface should let a human quickly scan thousands of images, filter by any dimension, and reject the ones with no potential. Only the survivors get enhanced.</p>
<p>Planned: thumbnail grid with filter pills (grading, vibe, time, setting, composition, exposure), keyboard-driven reject/keep workflow, progress tracker. Building once Gemini completes (~24h).</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>23:00 — The Blind Test Results <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("OpenCV 5, Pillow 5, Imagen 4, Skipped 6")</span><div class="ev-summary"><p><strong>Intent.</strong> Let the eyes decide, not the theory. A three-way tie with 30% rejected meant no method was good enough alone. The key insight: curation before enhancement. Don't waste effort improving images that have no potential.</p></div><div class="ev-body"><p><strong>Intent.</strong> Let the eyes decide, not the theory. A three-way tie with 30% rejected meant no method was good enough alone. The key insight: curation before enhancement. Don't waste effort improving images that have no potential.</p>
<p>Decision: wait for Gemini analysis on all 9,011 images, then build a curation interface to reject weak images before generating any edits. Enhancement approach TBD based on curated subset.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>22:45 — The Enhancement Showdown <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("The edits are not that good, there is often a white balance problem")</span><div class="ev-summary"><p><strong>Intent.</strong> The first 100 Imagen edits came back with persistent white balance issues. Imagen 3 is a generative model — it can't do precise color math. We needed to separate the deterministic work (white balance correction) from the creative work (exposure, contrast).</p></div><div class="ev-body"><p><strong>Intent.</strong> The first 100 Imagen edits came back with persistent white balance issues. Imagen 3 is a generative model — it can't do precise color math. We needed to separate the deterministic work (white balance correction) from the creative work (exposure, contrast).</p>
<p><strong>Discovered.</strong> Tested three approaches: Imagen with simplified prompts (guidance 30), OpenCV (GrayworldWB + CLAHE + auto gamma), and Pillow (grey world + autocontrast + brightness). All three produced decent but different results. None was clearly superior.</p>
<p>Built a blind test: 20 images, 4 columns (original + 3 shuffled enhancements), click your favorite. Served at <code>/blind-test</code>.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-pink)">UI/UX</span></div><h3>22:20 — Telling the Story <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">(Journal de Bord)</span><div class="ev-summary"><p><strong>Intent.</strong> This project is a process, not just a result. The decisions — why two-stage, why ask about rotation, why these 4 variants — are the story.</p></div><div class="ev-body"><p><strong>Intent.</strong> This project is a process, not just a result. The decisions — why two-stage, why ask about rotation, why these 4 variants — are the story.</p>
<p>Created this document. Served at <code>/journal</code>. Updated every session.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>22:15 — Tracking Imagen Progress <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("Add all this tracking to the monitor page")</span><div class="ev-summary"><p><strong>Intent.</strong> Three processes running simultaneously — Gemini analysis, gemini_edit, pro_edit — and no visibility into Imagen progress or rotation data.</p></div><div class="ev-body"><p><strong>Intent.</strong> Three processes running simultaneously — Gemini analysis, gemini_edit, pro_edit — and no visibility into Imagen progress or rotation data.</p>
<p>Added per-variant progress bars (success/failed/filtered) and rotation recommendation pills to dashboard.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>22:10 — First Visual Comparison <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("Once you have 100, open the folders for me to inspect")</span><div class="ev-summary"><p><strong>Intent.</strong> Before committing to 9,011 images worth of API calls, we need to see results. Are Gemini-guided edits actually better than generic ones? 100 is enough to judge.</p></div><div class="ev-body"><p><strong>Intent.</strong> Before committing to 9,011 images worth of API calls, we need to see results. Are Gemini-guided edits actually better than generic ones? 100 is enough to judge.</p>
<p>Launched gemini_edit + pro_edit for 100 images each, from 4K source. Visual comparison pending.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span><span class="ev-label" style="--label-color:var(--apple-teal)">Signal</span></div><h3>22:05 — Adding Rotation Detection <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("Add one question: should we rotate the image")</span><div class="ev-summary"><p><strong>Intent.</strong> Some photos are misoriented — EXIF data lost, scanned film upside down. Rather than a separate detection pass, ask Gemini while it's already looking.</p></div><div class="ev-body"><p><strong>Intent.</strong> Some photos are misoriented — EXIF data lost, scanned film upside down. Rather than a separate detection pass, ask Gemini while it's already looking.</p>
<p>Added <code>should_rotate</code> (none/cw90/ccw90/180) to prompt and DB. Restarted analysis for remaining 8,698 images.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>22:00 — Upgrading to 4K Source <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("Make sure we get the largest size")</span><div class="ev-summary"><p><strong>Intent.</strong> Imagen 3 outputs at input resolution. We were feeding 2048px, had 3840px available. Free quality upgrade.</p></div><div class="ev-body"><p><strong>Intent.</strong> Imagen 3 outputs at input resolution. We were feeding 2048px, had 3840px available. Free quality upgrade.</p>
<p>Changed source to full tier (3840px). All variants now 4K.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span><span class="ev-label" style="--label-color:var(--apple-orange)">Investigation</span></div><h3>21:55 — Naming the Four Variants <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("Why do you call it light_enhance?")</span><div class="ev-summary"><p><strong>Intent.</strong> Names matter. It's not a lighting fix anymore — it's a full Gemini-driven edit. And we want a second edit type for A/B comparison.</p></div><div class="ev-body"><p><strong>Intent.</strong> Names matter. It's not a lighting fix anymore — it's a full Gemini-driven edit. And we want a second edit type for A/B comparison.</p>
<p>Renamed to <code>gemini_edit</code> + <code>pro_edit</code>. Dropped cinematic/dreamscape. Final 4: gemini_edit, pro_edit, nano_feel, cartoon.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>21:50 — The Two-Stage Architecture <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("The cartoon could be better on an improved edited image")</span><div class="ev-summary"><p><strong>Intent.</strong> The key insight of the session. A cartoon of an underexposed, color-cast image inherits those problems. A cartoon of a properly edited image starts from a much better place. The Gemini analysis gives us image-specific editing instructions. Use those first, then build style variants on top.</p></div><div class="ev-body"><p><strong>Intent.</strong> The key insight of the session. A cartoon of an underexposed, color-cast image inherits those problems. A cartoon of a properly edited image starts from a much better place. The Gemini analysis gives us image-specific editing instructions. Use those first, then build style variants on top.</p>
<p>Rewired <code>imagen_engine.py</code> into two stages. Stage 1: edits from original (Gemini-guided + generic). Stage 2: styles from the enhanced result.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>21:40 — Evaluating the Next Phase <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("What can you work on next?")</span><div class="ev-summary"><p><strong>Intent.</strong> The Gemini analysis was going to take hours. Rather than wait, we wanted to understand what the next phases looked like. What's ready? What's blocked?</p></div><div class="ev-body"><p><strong>Intent.</strong> The Gemini analysis was going to take hours. Rather than wait, we wanted to understand what the next phases looked like. What's ready? What's blocked?</p>
<p><strong>Discovered.</strong> The Imagen engine had 5 hardcoded prompts and was completely ignoring Gemini's per-image editing advice. All that carefully generated <code>overall_edit_prompt</code> — thrown away. Also sourcing from 2048px when 3840px was available.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>21:30 — Seeing What the Machine Sees <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("I want to see one example of full data")</span><div class="ev-summary"><p><strong>Intent.</strong> Schemas are abstract. We wanted to see what the machine actually says about a photograph — the complete analysis, live, updating as new images are processed. This is how you build trust in the system.</p></div><div class="ev-body"><p><strong>Intent.</strong> Schemas are abstract. We wanted to see what the machine actually says about a photograph — the complete analysis, live, updating as new images are processed. This is how you build trust in the system.</p>
<p>Added "Sample Analysis" section to dashboard — full Gemini JSON, syntax-highlighted, refreshing every 5s.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>21:20 — Auditing the Database Schema <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("Did we get all the infos we wanted?")</span><div class="ev-summary"><p><strong>Intent.</strong> A gut check. The Gemini analysis was returning rich data — but was all of it actually being saved in a queryable way? If the data is buried in a raw JSON blob, you can't later ask "show me all photos with cinematic grading."</p></div><div class="ev-body"><p><strong>Intent.</strong> A gut check. The Gemini analysis was returning rich data — but was all of it actually being saved in a queryable way? If the data is buried in a raw JSON blob, you can't later ask "show me all photos with cinematic grading."</p>
<p><strong>Discovered.</strong> Three critical fields — <code>lighting_fix</code>, <code>color_fix</code>, and <code>overall_edit_prompt</code> — had no DB columns. The <code>overall_edit_prompt</code> was particularly important: it's the per-image instruction that would later drive the AI editor.</p>
<p>Added 3 columns, updated upsert, backfilled 156 rows from raw JSON. Restarted analysis.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>21:15 — Building the Live Dashboard <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("Build me a pretty minimal black and white web page")</span><div class="ev-summary"><p><strong>Intent.</strong> We needed to see what was happening. 9,011 images going through an AI analysis pipeline takes hours. Just watching a terminal scroll is useless — we wanted a dashboard that shows the big picture: how many images are done, how fast they're going, what the database looks like, what categories exist. Something clean, monospace, black and white. A control room.</p></div><div class="ev-body"><p><strong>Intent.</strong> We needed to see what was happening. 9,011 images going through an AI analysis pipeline takes hours. Just watching a terminal scroll is useless — we wanted a dashboard that shows the big picture: how many images are done, how fast they're going, what the database looks like, what categories exist. Something clean, monospace, black and white. A control room.</p>
<p>Created <code>generate_status_page.py</code> — live server mode (<code>--serve</code>) polls the DB every 5s. Stat cards, progress bar, category tables. All real-time.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>21:10 — Launching Gemini Analysis <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("Let's run all the images analysis now")</span><div class="ev-summary"><p><strong>Intent.</strong> The rendering pipeline had already processed all 9,011 images into a 6-tier resolution pyramid. The next step was the one that mattered most: having Gemini 2.5 Pro actually look at every photograph and understand it. Not metadata extraction — real visual analysis. What's the exposure doing? What draws the eye? What color palette dominates? What's the mood? And critically: what would a professional editor do to improve this specific image? We wanted structured, per-image intelligence that would later drive the AI editing.</p></div><div class="ev-body"><p><strong>Intent.</strong> The rendering pipeline had already processed all 9,011 images into a 6-tier resolution pyramid. The next step was the one that mattered most: having Gemini 2.5 Pro actually look at every photograph and understand it. Not metadata extraction — real visual analysis. What's the exposure doing? What draws the eye? What color palette dominates? What's the mood? And critically: what would a professional editor do to improve this specific image? We wanted structured, per-image intelligence that would later drive the AI editing.</p>
<p>Launched <code>photography_engine.py</code> on all 9,011 images. Gemini 2.5 Pro via Vertex AI, concurrency 5, exponential backoff with max 5 retries.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-purple)">AI</span></div><h3>20:30 — Wiring the AI Engines <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("Now the interesting part")</span><div class="ev-summary"><p><strong>Intent.</strong> Two AI engines: one that sees (Gemini 2.5 Pro for analysis) and one that edits (Imagen 3 for enhancement). The analysis engine studies each photograph and writes structured JSON: exposure, composition, color palette, mood, editing instructions. The editing engine uses those instructions to improve the image.</p></div><div class="ev-body"><p><strong>Intent.</strong> Two AI engines: one that sees (Gemini 2.5 Pro for analysis) and one that edits (Imagen 3 for enhancement). The analysis engine studies each photograph and writes structured JSON: exposure, composition, color palette, mood, editing instructions. The editing engine uses those instructions to improve the image.</p>
<p>Built <code>photography_engine.py</code> (Gemini analysis) and <code>imagen_engine.py</code> (Imagen editing with 4 variant types).</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span><span class="ev-label" style="--label-color:var(--apple-pink)">UI/UX</span></div><h3>20:00 — The Rendering Pyramid <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("We need different sizes for different uses")</span><div class="ev-summary"><p><strong>Intent.</strong> A 40MB RAW scan is useless for a thumbnail grid. A 200px thumb is useless for printing. We needed a pyramid: thumb (200px) for grid navigation, micro (480px) for previews, mobile (1080px) for phones, display (1920px) for screens, full (3840px) for AI processing and printing.</p></div><div class="ev-body"><p><strong>Intent.</strong> A 40MB RAW scan is useless for a thumbnail grid. A 200px thumb is useless for printing. We needed a pyramid: thumb (200px) for grid navigation, micro (480px) for previews, mobile (1080px) for phones, display (1920px) for screens, full (3840px) for AI processing and printing.</p>
<p>6 tiers × 9,011 images = 54,066 rendered files. ~52 GB total. Each tier in JPEG format with quality appropriate to its purpose.</p></div></div>
<div class="event ev-collapsed" onclick="this.classList.toggle('ev-collapsed')"><div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-brown, #a2845e)">Infrastructure</span><span class="ev-label" style="--label-color:var(--apple-blue)">Pipeline</span></div><h3>19:00 — Laying the Foundation <span class="ev-expand-hint">&#9656;</span></h3><span class="quote">("Build me a pipeline")</span><div class="ev-summary"><p><strong>Intent.</strong> The starting point: 11,557 photographs in a folder, organized by medium — Analog, Digital, Monochrome, Osmo, G12. No metadata, no organization beyond folders. The goal: build a pipeline that can process every single image through AI analysis and enhancement. That meant: database schema, UUID generation, file registration, and a multi-tier rendering system.</p></div><div class="ev-body"><p><strong>Intent.</strong> The starting point: 11,557 photographs in a folder, organized by medium — Analog, Digital, Monochrome, Osmo, G12. No metadata, no organization beyond folders. The goal: build a pipeline that can process every single image through AI analysis and enhancement. That meant: database schema, UUID generation, file registration, and a multi-tier rendering system.</p>
<p>Built <code>mad_database.py</code> (SQLite schema), <code>render_pipeline.py</code> (6-tier resolution pyramid), <code>mad_pipeline.py</code> (orchestrator). Registered 9,011 images. Rendered all tiers: thumb, micro, mobile, display, full, original.</p></div></div>
<h2 class="date-header">Origin</h2>
<div class="event event-genesis">
<div class="ev-labels"><span class="ev-label" style="--label-color:var(--apple-indigo)">Genesis</span></div>
<h3>The Vision</h3>
<p class="intent">9,011 unedited photographs taken over a decade with five cameras. The mission: augment every single image with every possible signal — AI analysis, pixel metrics, vector embeddings, depth maps, scene classification, object detection, face emotions, captions, color palettes. Then enhance each frame with camera-aware, signal-driven corrections.</p>
<p>Three apps, one pipeline. <strong>Show</strong> — blow people's minds with experiences that are playful, elegant, smart, teasing, revealing. Continuously release new ways to see photographs, guided by signals and new ideas. <strong>State</strong> — the dashboard, the control room. Every signal, every model, every image tracked. <strong>See</strong> (MADCurator) — the native power image viewer. 55 fields, 18 filters, full-resolution. The human eye decides what's worth showing.</p>
<blockquote>We started with 9,011 raw images and zero metadata. We will create the best experience on photos. Game ON.</blockquote>
</div>
<footer>MADphotos &mdash; <a href="https://github.com/LAEH/MADphotos">github.com/LAEH/MADphotos</a></footer>
</div>
<script>
(function() {
  function getTheme() { return localStorage.getItem('mad-theme') || 'light'; }
  function applyTheme(t) {
    document.documentElement.setAttribute('data-theme', t);
    var icon = document.getElementById('themeIcon');
    var label = document.getElementById('themeLabel');
    if (icon) icon.textContent = t === 'dark' ? '\u2600' : '\u263E';
    if (label) label.textContent = t === 'dark' ? 'Light Mode' : 'Dark Mode';
  }
  window.toggleTheme = function() {
    var t = getTheme() === 'dark' ? 'light' : 'dark';
    localStorage.setItem('mad-theme', t);
    applyTheme(t);
  };
  window.toggleSidebar = function() {
    document.body.classList.toggle('sb-collapsed');
    localStorage.setItem('mad-sidebar', document.body.classList.contains('sb-collapsed') ? 'collapsed' : 'expanded');
  };
  applyTheme(getTheme());
  if (localStorage.getItem('mad-sidebar') === 'collapsed') {
    document.body.classList.add('sb-collapsed');
  }
})();
</script>

</body>
</html>